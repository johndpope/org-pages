# -*- mode: org; -*-
#+OPTIONS: toc:2
#+OPTIONS: ^:nil
#+SETUPFILE: setup.org
#+TITLE: note-linux-centos

vbox自建centos集群环境，方便本地开发测试,

* VboxManage
   
  - vms 基本信息
    
    VBoxManage showvminfo Centos701

  - 无界面启动vms

    VBoxManage startvm Centos701 --type headless

    VBoxManage controlvm ubuntu01 savestate # 保存当前状态关闭

  - 修改vms内存总量

    VBoxManage modifyvm Centos701 --memory 1024
    
  - 修改vms磁盘容量

    VBoxManage modifyhd Centos702.vdi --resize 7168

    #+BEGIN_SRC 
   
如果创建过快照，快照的磁盘空间也需要修改
VBoxManage modifyhd \{aea51811-5e79-46e9-b5ba-f66740e6f47b\}.vdi  --resize 10248
    
    #+END_SRC
    
  - 创建快照
    
    VBoxManage snapshot ubuntu01 take ubuntu_basic_ssh

    VBoxManage snapshot ubuntu01 list

    VBoxManage snapshot ubuntu01 delete ubuntu_basic

    VBoxManage snapshot ubuntu01 restore ubuntu_basic_ssh

  - clone vm

    VBoxManage clonevm  Centos701 --snapshot  centos7_basic  --name Centos_basic

#+BEGIN_SRC 

查看当前虚拟机  VBoxManage list vms  

查看当前正在运行的虚拟机  VBoxManage list runningvms  

启动虚拟机  VBoxManage startvm 虚拟机名  

无前端图形界面方式启动虚拟机  VBoxManage startvm 虚拟机名 --type headless  

#+END_SRC

* centos7 basic environment
  centos7 镜像[[http://isoredirect.centos.org/centos/7/isos/x86_64/CentOS-7-x86_64-Minimal-1708.iso][下载]]

** lvm 
   在文件中添加要挂载的分区和文件目录可以修改文件
   
   /etc/fstab

   /dev/sda5/    media/win    ntfs    defaults   02

   然后 mount -a

   1. 查看几块硬盘
   
      sudo fdisk -l |grep sd
   
   2. 创建分区
   
      虚拟机现有20g的硬盘,使用fdisk划分磁盘
       
      sudo fdisk /dev/sda
   
      #+BEGIN_VERSE
         m  帮助信息  
         n 创建分区
         e 扩展分区    +5G  pppp/pppe
         p 打印分区
         t 分区类型 L  (lvm)
         w 写入保存分区
      #+END_VERSE

   3. 格式化 分区

   - LVM
     pv --> vg --> lv
     参考:
     http://blog.sina.com.cn/s/blog_b77735d20101e5cn.html
     http://aurthurxlc.github.io/Aurthur-2017/Centos-7-extend-lvm-volume.html

     #+BEGIN_SRC 
     fdisk -l | grep sd
     fdisk /dev/sda
     partprobe
     pvdisplay
     pvcreate /dev/sda3
     vgdisplay
     vgextend centos /dev/sda3
     lvdisplay
     #lvcreate -L 3.31G -n manue1 centos
     #mkfs.xfs /dev/centos/manue1
     #lvremove -f /dev/centos/manue1
     lvextend -l +100%FREE /dev/centos/root
     df -Th
     xfs_growfs /dev/centos/root
     #+END_SRC
** ip
  * 联网方式: 配置三张网卡
    virtualBox 中 NAT 和 Host-only 两种模式不能同时并存，测试只能联网的时候关掉另一张网卡
    1. NAT 
      网卡1 用来连接外网 
    2. Host-only
      用来配置静态IP 配置集群服务的时候不需要修改IP
      vi /etc/sysconfig/network-scripts/  
       #+BEGIN_SRC 
         #static assignment
         ONBOOT=yes
         BOOTPROTO=static
         IPADDR=192.168.56.10
         NETMASK=255.255.255.0
         GATEWAY=192.168.56.1
       #+END_SRC
    3. Bridge
      vbox 自动配置IP，也很方便 
       
    这边打算使用网卡1 nat模式连接外网，网卡3的桥接模式与局域网内其他主机通信,网卡二的主机模式搭建集群

    注意： 网卡二和网卡三的 gateway 字段要注释掉
    
  * ip tool

    Ip  [选项]  操作对象{link|addr|route...}
    
    # ip link show                           # 显示网络接口信息
    
    # ip link set eth0 up                   # 开启网卡
    
    # ip link set eth0 down                  # 关闭网卡
    
    # ip link set eth0 promisc on            # 开启网卡的混合模式
    
    # ip link set eth0 promisc offi          # 关闭网卡的混个模式
    
    # ip link set eth0 txqueuelen 1200       # 设置网卡队列长度
    
    # ip link set eth0 mtu 1400              # 设置网卡最大传输单元
    
    # ip addr show                           # 显示网卡IP信息
    
    # ip addr add 192.168.0.1/24 dev eth0    # 设置eth0网卡IP地址192.168.0.1
    
    # ip addr del 192.168.0.1/24 dev eth0    # 删除eth0网卡IP地址
    
    # ------
    
    # ip route list                                            # 查看路由信息
    
    # ip route add 192.168.4.0/24  via  192.168.0.254 dev eth0 # 设置192.168.4.0网段的网关为192.168.0.254,数据走eth0接口
    
    # ip route add default via  192.168.0.254  dev eth0        # 设置默认网关为192.168.0.254
    
    # ip route del 192.168.4.0/24                              # 删除192.168.4.0网段的网关
    
    # ip route del default                                     # 删除默认路由
      
    sudo service network restart
    
** sshd
  ssh 连接异常慢
  sudo vi /etc/ssh/sshd_config

  #+BEGIN_SRC 
  UseDNS no
  #+END_SRC

** hostname
  永久修改主机名字
  sudo hostnamectl --static set-hostname master

  sudo vi /etc/hosts

  [manue1@localhost ~]$ cat /etc/hostname
   master
  [manue1@localhost ~]$ cat /etc/hosts
   127.0.0.1 master
   ::1 master
** yum source

  sudo yum -y install wget
  
  - 备份
    sudo mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup

  - 设置aliyun source
    sudo wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo

  - 设置EPLEPEL source
    
    sudo wget -P /etc/yum.repos.d/ http://mirrors.aliyun.com/repo/epel-7.repo

    添加后可以像fedora上 yum install packname

  - 清理缓存并生成新的缓存

    sudo yum clean all  
    sudo yum makecache  

** ntp

   | master  | server |  
   |---------+--------+
   | slave01 | client |  
   | slave02 | client |

  - any nodes

    sudo yum -y install ntp
    
    timedatectl set-timezone Asia/Shanghai   # 设置上海时区
   
   - server configural
     
     systemctl start ntpd
      
     systemctl enable ntpd

     vi /etc/ntp.conf 
     
     #+BEGIN_SRC 
     
restrict 192.168.56.0 mask 255.255.0.0

server 127.127.1.0

fudge 127.127.1.0 stratum 10
     #+END_SRC
     
     systemctl restart ntpd

   - client configural
     
     systemctl start ntpd
     
     systemctl enable ntpd

     vi /etc/ntp.conf
     
     #+BEGIN_SRC 
server 192.168.56.10
     
     #+END_SRC

    netstat -anp | grep 123

** firewallds
  
  - 查看状态
    systemctl status firewalld
  - 关闭
    systemctl stop firewalld
  - 禁用
    systemctl disable firewalld
** disable selinux
  一款为了提高系统安全性的软件：对系统服务，文件权限，网络端口访问有极其严格的限制，
  例如：如果对一个文件没有正确安全上下文配置， 甚至你是root用户，你也不能启动某服务

  sudo vi /etc/sysconfig/selinux
   selinux = disable
** java  & scala
  基础环境用root 配置在/etc/profile 自启动环境文件内
  refer : https://www.mtyun.com/library/how-to-setup-scala-on-centos7
  - java rpm install

    1. download
       http://www.oracle.com/technetwork/java/javase/downloads/index.html
    2. install
       sudo rpm -ivh jdk-8u144-linux-x64.rpm
       sudo rpm -aq | grep jdk
       
       sudo rpm -e jdk   无效
       sudo yum remove jdk  
       
       sudo vi /etc/profile
       #+BEGIN_SRC 
         #JAVA_HOME
         export JAVA_HOME=/usr/java/jdk1.8.0_144
         export CLASSPATH=.:$JAVA_HOME/jre/lib/rt.jar:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
         export PATH=$PATH:$JAVA_HOME/bin
       #+END_SRC
  - java 离线包安装

    tar -zxvf jdk-8u151-linux-x64.tar.gz

    vi /etc/profile
    #+BEGIN_SRC 
#JAVA_HOME
JAVA_HOME=/home/manue1/opt/jdk8
PATH=$PATH:$JAVA_HOME/bin
export JAVA_HOME PATH
    #+END_SRC
  - scala 离线包安装
    当前最新版本
    tar zxvf scala-2.11.7.tgz

    #+BEGIN_SRC 
    SCALA_HOME=/home/manue1/opt/scala-2.11.7
    PATH=$PATH:$SCALA_HOME/bin
    export SCALA_HOME PATH
    #+END_SRC
    
* hadoop 集群配置
** hadoop hbase spark 版本选择
   + hbase 支持 hadoop 版本对照表
   
      The 1.2.x series is the current stable release line
     
      http://www-us.apache.org/dist/hbase/
     
      下面查看1.2.x 需要的hadoop版本
   
      http://hbase.apache.org/book.html#arch.overview
   
      crtl + F  "s" 搜索页面
   
     选择 Hadoop-2.7.1+
     
   + spark 支持 hadoop
     
     http://spark.apache.org/downloads.html
     
     官方下载页面可以手动选择
   
   + hive 支持 hadoop
     
     https://hive.apache.org/downloads.html
   
     稳定版下载地址
     
     http://mirrors.shuosc.org/apache/hive/stable-2/
   
   + zookeeper
       
       下载稳定版即可
       
       http://mirrors.shuosc.org/apache/zookeeper/stable/

** 环境准备
   三台vbox 虚拟centos7 配置 java scala 环境 关闭防火墙和selinux
   
   - cluster
     | hostname |            ip |
     |----------+---------------|
     | master   | 192.168.56.10 |
     |----------+---------------|
     | slave01  | 192.168.56.11 |
     |----------+---------------|
     | slave02  | 192.168.56.12 |
     |----------+---------------|

   - disable ipv6

     sudo vi /etc/sysctl.conf
     
     添加下面内容
     
     #+BEGIN_SRC 

# disable ipv6
net.ipv6.conf.all.disable_ipv6 = 1
net.ipv6.conf.default.disable_ipv6 = 1
net.ipv6.conf.lo.disable_ipv6 = 1
     
     #+END_SRC

     解决master:50070 页面找不到live node 

     解决 connection exception
     
     #+BEGIN_SRC 
17//23 23:19:48 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
ls:all From slave01/127.0.0.1 to master:9000 failed on connection exception: java.net.ConnectException: 拒绝连接; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
     
     #+END_SRC

   - hostname & host
     三台主机都要
     修改主机名
     修改/etc/hosts 互相添加hostname访问别名
     注意； #127.0.0.1 master 这样的映射一定要注释掉,master:8088无法访问最终定位到这里了
     #+BEGIN_SRC 
#ceos7 cluster
19268.56.10 master
19268.56.11 slave01
19268.56.12 slave02
     #+END_SRC

   - 免登录验证
     ssh-keygen -t rsa
     ssh-copy-id -i ~/.ssh/id_rsa.pub manue1@slave01 
     ssh-copy-id -i ~/.ssh/id_rsa.pub manue1@slave02 
     ssh-copy-id -i ~/.ssh/id_rsa.pub manue1@master
     
   - download hadoop
       tar -zxvf hadoop-2.7.5.tar.gz
** 配置hadoop cluster
     
    - hadoop_home
      三台节点都需要配置
      vi .bashrc
      #+BEGIN_SRC 
# Hadoop Environment Variables
export HADOOP_HOME=/home/manue1/opt/hadoop-2.7.5
export HADOOP_INSTALL=$HADOOP_HOME
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin
export HADOOP_OPTS="$HADOOP_OPTS -Djava.library.path=$HADOOP_HOME/lib/native" #解决WARN util.NativeCodeLoader: Unable to load native-hadoop library
export CATALINA_BASE=$HADOOP_HOME/share/hadoop/httpfs/tomcat #支持httpfs rest api
      
      #+END_SRC

    - master
      
      /home/manue1/opt/hadoop-2.7.5/etc/hadoop/ 下6个配置文件
      1. core-site.xml

         #+BEGIN_SRC 
<configuration>
    <!-- 指定HDFS老大（namenode）的通信地址 -->
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://master:9000</value>
    </property>
    <!-- 指定hadoop运行时产生文件的存储路径 -->
    <property>
        <name>hadoop.tmp.dir</name>
        <value>file:/home/manue1/opt/hadoop-2.7.5/tmp</value>
    </property>
</configuration>
    <!--开启httpfs实现一种匿名的方式登陆hdfs文件系统 端口14000
        manue1用户为hdfs的超级用户 hive启动用户
    -->

    <property>
         <name>hadoop.proxyuser.manue1.hosts</name>
         <value>*</value>
    </property>
    <property>
        <name>hadoop.proxyuser.manue1.groups</name>
        <value>*</value>
    </property>

         
         #+END_SRC

      2. hdfs-site.xml
         
         #+BEGIN_SRC 
<configuration>
        <!-- 设置namenode的http通讯地址 -->
        <property>
                <name>dfs.namenode.secondary.http-address</name>
                <value>master:50090</value>
        </property>
        <!-- 设置hdfs副本数量 -->
        <property>
                <name>dfs.replication</name>
                <value>1</value>
        </property>
         <!-- 设置namenode存放的路径 -->
        <property>
                <name>dfs.namenode.name.dir</name>
                <value>file:/home/manue1/opt/hadoop-2.7.5/tmp/dfs/name</value>
        </property>
         <!-- 设置datanode存放的路径 -->
        <property>
                <name>dfs.datanode.data.dir</name>
                <value>file:/home/manue1/opt/hadoop-2.7.5/tmp/dfs/data</value>
        </property>
</configuration>

         
         #+END_SRC

      3. mapred-site.xml
         
         mv mapred-site.xml.template mapred-site.xml

         #+BEGIN_SRC 
<configuration>
        <!-- 通知框架MR使用YARN -->
        <property>
                <name>mapreduce.framework.name</name>
                <value>yarn</value>
        </property>
        <property>
                <name>mapreduce.jobhistory.address</name>
                <value>master:10020</value>
        </property>
        <property>
                <name>mapreduce.jobhistory.webapp.address</name>
                <value>master:19888</value>
        </property>
</configuration>
         
         #+END_SRC
         
      4. yarn-site.xml
         
         #+BEGIN_SRC 
<configuration>
 <!-- 设置 resourcemanager 在哪个节点-->
<!-- Site specific YARN configuration properties -->
        <property>
                <name>yarn.resourcemanager.hostname</name>
                <value>master</value>
        </property>
         <!-- reducer取数据的方式是mapreduce_shuffle -->
        <property>
                <name>yarn.nodemanager.aux-services</name>
                <value>mapreduce_shuffle</value>
        </property>
        <!--所有主机访问yarn管理界面-->
        <property> 
                <name>yarn.resourcemanager.webapp.address</name>
                <value>0.0.0.0:8088</value>
        </property>

</configuration>
         
         #+END_SRC

      5. slaves
         
         #+BEGIN_SRC 
         slave01
         slave02
         #+END_SRC

      6. hadoop-env.sh

         修改
         export JAVA_HOME=/home/manue1/opt/jdk8
         
    - slaves
      
      复制master节点配置好的安装包到指定slaves目录
      
      : tar -zcvf hadoop-2.7.5_conf_finshed.tar.gz hadoop-2.7.5/
      : scp hadoop-2.7.5_conf_finshed.tar.gz manue1@slave02:/home/manue1/opt/

** 启动hadoop

   第一次启动要执行格式化，之后启动不用执行这条
   : hdfs namenode -format 
   
   启动命令:
   : start-dfs.sh
   : start-yarn.sh
   : mr-jobhistory-daemon.sh start historyserver  ??
   : httpfs.sh start


   - master
       #+BEGIN_SRC 
manue1@master sbin]$ jps
2034 NameNode
2483 Jps
15754 Bootstrap  #httpfs
1652 ResourceManager
2188 SecondaryNameNode
2447 JobHistoryServer
       
       #+END_SRC
       
   - slaves
       #+BEGIN_SRC 
[manue1@slave01 hadoop]$ jps
1360 DataNode
1430 NodeManager
1516 Jps
       #+END_SRC

  hadoop cluster状态展示界面 webhdfs

  : http://master:50070/
  : curl "http://master:50070/webhdfs/v1/?op=liststatus&user.name=manue1"
  
  httpfs rest api 配置HA的时候找不到namenode可以采用httpfs

  : http://master:14000/
  : curl "http://master:14000/webhdfs/v1/?op=liststatus&user.name=manue1"

  yarn 管理界面

  : http://master:8088

* elasticstk 集群
** elasticsearch-611
   #+BEGIN_QUOTE
   1. 环境准备
        
     | hostname |            ip |            |
     |----------+---------------+------------|
     | master   | 192.168.56.10 | masternode |
     |----------+---------------+------------|
     | slave01  | 192.168.56.11 | datanode   |
     |----------+---------------+------------|
     | slave02  | 192.168.56.12 | datanode   |
      
     -  生产环境配置: 

        10节点 6个master候选节点

        1节点 4G + lucene 3G = 8G

        10T硬盘，最多存5T数据

        4c + 8g + 1T * 10台 = 10T 
        4c + 16g + 2T * 5台 = 10T

     - java 环境配置，关闭firewalld,主机名配置,官网下载elasticsearch-6.1.1.tar.gz

     - 普通用户下安装es  username:manue1
       
     - 系统设置

      sudo -s 切换到root下执行

      #+BEGIN_SRC 

sed -e '$a vm.max_map_count = 262144' -i /etc/sysctl.conf

sysctl -p

 

echo "ulimit -SHn 1048576" >> /etc/rc.local

sed -e '$a DefaultLimitCORE=infinity\nDefaultLimitNOFILE=1048576\nDefaultLimitNPROC=1048576' -i /etc/systemd/system.conf

cat >> /etc/security/limits.conf << EOF

 *           soft   nofile       1048576

 *           hard   nofile       1048576

 *           soft   nproc        1048576

 *           hard   nproc        1048576

EOF

sed -i 's/4096/1048576/' /etc/security/limits.d/20-nproc.conf

sed -e '/root       soft    nproc     unlimited/a\*           soft   nofile       1048576\n*           hard   nofile       1048576' -i /etc/security/limits.d/20-nproc.conf
      
      #+END_SRC
      修改系统配置文件后，重启系统生效
      
   2. 配置elasticsearch
      
     - elasticsearch.yml          # els的配置文件
       
       #+BEGIN_SRC 
cluster.name: manue1-es-cluster  #集群名称

node.name: master-node           #节点名称

node.data: false
node.master: true  #建议直接不设置，默认两个都为true.

path.data: /Home/Manue1/Opt/Elasticsearch-6.1.1/Els/Data  #数据存储目录

path.logs: /home/manue1/opt/elasticsearch-6.1.1/els/log   #日志存储目录

network.bind_host: 0.0.0.0   #master节点配置 ”0.0.0.0”，允许所有网络接口访问
network.publish_host: master # 集群通信

gateway.recover_after_nodes: 3  #值为n，网关控制在n个节点启动之后才恢复整个集群, 3节>点启动后1分钟
gateway.recover_after_time: 1m

indices.recovery.max_bytes_per_sec: 20mb  #恢复数据时,限制的宽带流量,如果是0就是无限制

node.max_local_storage_nodes: 1                  #值为n，一个系统中最多启用节点个数为n

http.port: 9200                 # 对外提供服务的端口，9300为集群服务的端口


       #+END_SRC
       
     - jvm.options                # JVM相关的配置，内存大小等等

       #+BEGIN_SRC 
      -Xms128M
      -Xmx128M
      -Xmx1g与-Xms1gJVM的最大最小内存。如果太小会导致Elasticsearch刚刚启动就立刻停止。太大会拖慢系统本身
       #+END_SRC

     - log4j2.properties          # 日志系统定义

     将配置好的elasticsearch 打包传到各个节点，需要注意的是，如果配置过程中运行产生的data/nodes/0 文件
     一定要删掉，再打包使用，否则各个节点启动成功了，无法加入到集群，节点id冲突
     报错信息: =with the same id but is a different node instance=
      
   3. 启动elasticsearch
      
      su manue1

      vi  /home/manue1/opt/elasticsearch-6.1.1/bin/elasticsearch

      #+BEGIN_SRC 
ES_HEAP_SIZE=128m
MAX_OPEN_FILES=262144
      #+END_SRC
      
     nohup ./bin/elasticsearch -d 

     关闭 ps -ef |grep elasticsearch|awk '{print $2}'|xargs kill -9

   4. refer

      [[https://blog.csdn.net/thomas0yang/article/details/55518105#%E5%86%85%E5%AD%98][1]] [[https://www.elastic.co/guide/cn/elasticsearch/guide/cn/important-configuration-changes.html#_%E6%9C%80%E5%B0%8F%E4%B8%BB%E8%8A%82%E7%82%B9%E6%95%B0][2]] [[https://zhuanlan.zhihu.com/p/35291900][3]]

`
   #+END_QUOTE
   
** kibana

   配置在es的非数据节点上: 192.168.56.10

   修改 config/kibana.yml
   #+BEGIN_SRC 
server.host: "0.0.0.0" #不同网卡网段能够访问
elasticsearch.url: "http://master:9200"
   #+END_SRC

   启动： nohup  bin/kibana  &

   关闭: ps -ef |grep kibana |awk '{print $2}'|xargs kill -9

   ss -lnp | grep 5601

** logstash
   1. download
      logstash-6.1.1.tar.gz
   2. config
      
      - 创建logstash-conf 目录
        beat的配置文件
        vi beats.conf
        #+BEGIN_SRC 
input {
  beats {
    port => 5044
  }
}

# The filter part of this file is commented out to indicate that it is
# optional.
# filter {
#
# }

output {
  elasticsearch {
    hosts => "master:9200"
    manage_template => false
    index => "%{[@metadata][beat]}-%{[@metadata][version]}-%{+YYYY.MM.dd}" 
  }
}
        
        #+END_SRC
        
      - jvm.options
        修改 xms xmx 最大最小jvm 为256M 比es测试集群吃内存多 

      - logstash.yml

   3. start logstash

     -  bin/logstash -e 'input { stdin { } } output { stdout {} }'
        测试启动

     - ./bin/logstash -f logstash-conf/beats.conf &
       
       配置文件启动

       sudo netstat -anp | grep 5044
** beats
*** topbeat
    5.x版本后弃用了
    1. 下载
       topbeat-1.3.1-x86_64.tar.gz

    2. 配置

      - topbeat

      - topbeat.template.json  
        topbeat自带的模版，用来创建存放收集数据的索引结构

      - topbeat.yml
        #+BEGIN_SRC 
input:
  period: 10           #默认10秒收集一次
  procs: [".*"]   #定义正则表达式，以匹配你所要监控的进程。默认是所有正在运行的进程都进行监控。
  stats:
    system: true
    proc: true
    filesystem: true
output:
  elasticsearch:
    hosts: ["master:9200"]
shipper:
logging:
  files:
        #+END_SRC
   

    3. es导入模版
       导入topbeat自带的模版，用来创建存放收集数据的索引结构

       - Configuring Template Loading - supported for Elasticsearch output only
         #+BEGIN_SRC 
         ERR Failed to perform any bulk index operations: 406 Not Acceptable
         错误应该是模版和6.0版本不匹配了，官网没有更新
         再去官网查看，topbeat 从5.0 已经被 Metricbeat替换了
         #+END_SRC

       - Loading the Template Manually - required for Logstash output


    4. kibana 

    5. 启动topbeat节点

       sudo ./topbeat -e -c topbeat.yml -d "publish"
*** filebeat
    1. download

       filebeat-6.1.1-linux-x86_64.tar.gz

       https://download.elastic.co/demos/logstash/gettingstarted/logstash-tutorial.log.gz
       logstash-tutorial.log.gz apache 的日志文件样本

    2. config
       
       - filebeat.yml
         #+BEGIN_SRC 
- type: log
  # Change to true to enable this prospector configuration.
  enabled: true
  # Paths that should be crawled and fetched. Glob based paths.
  paths:
    - /var/log/*.log
    - /home/manue1/opt/source/*.log
    #- c:\programdata\elasticsearch\logs\*


output.logstash:
  # The Logstash hosts
  hosts: ["master:5044"]

         
setup.kibana:

  host: "master:5601"         

         #+END_SRC
         
       - modules
         
         sudo chown -R root /home/manue1/opt/filebeat-6.1.1-linux-x86_64/module

         sudo chown -R root /home/manue1/opt/filebeat-6.1.1-linux-x86_64/modules.d
         
         - Enable modules when you run Filebeatedit

           sudo ./filebeat -e --modules system,nginx,mysql  
         
           ./filebeat -e --modules nginx -M "nginx.access.var.paths=[/var/log/nginx/access.log*]"
           
         - filebeat.yml

           sudo ./filebeat modules list

           sudo ./filebeat modules enable system 

           #+BEGIN_SRC 
默认配置读取所有enable
filebeat.modules:
- module: nginx
- module: mysql
- module: system
           
           #+END_SRC

           

         

       - setup template
         
         for logstash manually setup
         #+BEGIN_SRC 
./filebeat setup --template -E output.logstash.enabled=false -E 'output.elasticsearch.hosts=["localhost:9200"]'
         #+END_SRC

       - setup kibana dashboards

         #+BEGIN_SRC 
         ./filebeat setup --dashboards
         #+END_SRC

       - start filebeat

         sudo chown root filebeat.yml 

         sudo -s

         nohup /home/manue1/opt/filebeat-6.1.1-linux-x86_64/filebeat -e -c /home/manue1/opt/filebeat-6.1.1-linux-x86_64/filebeat.yml -d "publish" &

         ps aux |grep beat

*** metricbeat
    
    1. download
       
       metricbeat-6.1.1-linux-x86_64.tar.gz
       
    2. conf
       
       - metricbeat.yml
         
        1. 修改es和kibana的地址
           
           如果输出到logstash中，需要关闭直接写入es，并配置logstash监听5044端口
           es也要手动加载template
           #+BEGIN_SRC 
output.logstash:
  # The Logstash hosts
  hosts: ["master:5044"]
           #+END_SRC
        
        2. 配置template模版
           - Configure template loading
           - Load the template manually  
             required for Logstash output
             #+BEGIN_SRC 
             
             sudo ./metricbeat setup --template -E output.logstash.enabled=false -E 'output.elasticsearch.hosts=["master:9200"]'
             #+END_SRC

             
       - modules.d 
         目录下面可以配置多种模块
         修改 logstash.yml.disabled 为 logstash.yml 启动模块

       - kibana dashboard

         ./metricbeat setup --dashboards
         
    3. start 
       
       sudo chown root metricbeat.yml 
       sudo chown root modules.d/system.yml 
       sudo ./metricbeat -e -c metricbeat.yml -d "publish"

       ps aux |grep metricbeat


    复制到不同节点部署
*** packetbeat
    1. download 
       
       packetbeat-6.1.1-linux-x86_64.tar.gz

    2. config 
       
       - packetbeat.yml
         
         logstash & kibana 地址修改

       - setup template
         
         ./packetbeat setup --template -E output.logstash.enabled=false -E 'output.elasticsearch.hosts=["master:9200"]'

       - set up kibana dashboard

         ./packetbeat setup --dashboards

    3. start beat
       
       sudo chown root packetbeat.yml 

       nohup /home/manue1/opt/packetbeat-6.1.1-linux-x86_64/packetbeat -e -c  /home/manue1/opt/packetbeat-6.1.1-linux-x86_64/packetbeat.yml -d "publish" &
       
* hive 

hive只需要安装在集群任意一个节点上即可,这里安装在slave01上
  
** install mariadb

   安装hive前，需要mysql作为外置存储引擎，存放hive元数据(metastore)

   [[http://blog.csdn.net/Nemo____/article/details/72897455][参考安装]] mysql准备环境

   - remove mariadb
     : rpm -qa|grep mariadb         //查询出已安装的mariadb
     : rpm -e --nodeps 文件名        //卸载 ， 文件名为使用rpm -qa|grep mariadb 命令查出的所有文件
     : sudo  rpm -e --nodeps mariadb-libscc
         
   - install mariadb

    : yum install mariadb-server mariadb
    : systemctl start mariadb  #启动MariaDB
    : systemctl stop mariadb   #停止MariaDB
    : systemctl restart mariadb  #重启MariaDB
    : systemctl enable mariadb  #设置开机启动
    : mysql -uroot -p #NO PASSWORD
    : set password for 'root'@'localhost' =password('manue1');  # set new password
    : grant all privileges on *.* to root@'%'identified by 'manue1';  #远程连接设置

    vi /etc/my.cnf
    #+BEGIN_SRC 
    # set utf8
    [mysql]
    default-character-set=utf8  # NO SPACE
   #+END_SRC

** config hive

 配置安装[[http://mirrors.shuosc.org/apache/hive/stable-2/][hive]] 参考： [[https://cwiki.apache.org/confluence/display/Hive/GettingStarted][1]]  [[http://blog.csdn.net/jssg_tzw/article/details/72354470][2]]

- *.bashrc*

 hive 环境变量配置

 : export HIVE_HOME=/home/manue1/opt/apache-hive-2.3.2-bin
 : export PATH=$PATH:$HIVE_HOME/bin

- *metastore conf*

 hive元数据存放mysql,为hive建立相应的mysql账户,并赋予足够的权限
     
 : mysql -h slave01 -uroot -p
 : insert into mysql.user (Host,User,Password)values('localhost','hive',password('manue1'));
 : create database hive;
 : grant all privileges on hive.* to hive@'%'identified by 'manue1'; 
 : flush privileges; 

- *配置hive-env.sh 文件*

   mv hive-env.sh.template hive-env.sh
  
  : export HADOOP_HOME=/home/manue1/opt/hadoop-2.7.5
  : export HIVE_CONF_DIR=/home/manue1/opt/apache-hive-2.3.2-bin/conf
  : export HIVE_AUX_JARS_PATH=/home/manue1/opt/apache-hive-2.3.2-bin/lib

- *hive-site.xml*
      
 : mv hive-default.xml hive-site.xml
      
 1. hdfs新建hive数据目录

    因为在hive-site.xml配置了hive表的数据存放在hdfs上的/user/hive/warehouse内,
   
    : <name>hive.metastore.warehouse.dir</name>
    : <value>/user/hive/warehouse</value>
    : <name>hive.exec.scratchdir</name>
    : <value>/tmp/hive</value>
   
    所以要在Hadoop集群新建目录，执行命令
   
    : [manue1@master conf]$ hadoop fs -mkdir -p /user/hive/warehouse
    : [manue1@master conf]$ hadoop fs -chmod -R 777 /user/hive/warehouse
    : [manue1@master conf]$ hadoop fs -mkdir -p /tmp/hive
    : [manue1@master conf]$ hadoop fs -chmod -R 777 /tmp/hive

 2. hive-site.xml内mysql相关配置

    需要java连接mysql的依赖包下载[[https://dev.mysql.com/downloads/connector/j/5.1.html][mysql-connector-java-5.1.45-bin.jar]] 
   
    : mv mysql-connector-java-5.1.45-bin.jar lib/
   
   #+BEGIN_SRC 
    1. javax.jdo.option.ConnectionDriverName，将该name对应的value修改为MySQL驱动类路径：
    <property
      <name>javax.jdo.option.ConnectionDriverName</name>
      <value>com.mysql.jdbc.Driver</value>
    </property>  
   
    2. javax.jdo.option.ConnectionURL，将该name对应的value修改为MySQL的地址：
     <name>javax.jdo.option.ConnectionURL</name>
     <value>jdbc:mysql://192.168.56.11:3306/hive?createDatabaseIfNotExist=true</value>
   
    3.javax.jdo.option.ConnectionUserName，将对应的value修改为MySQL数据库登录名：
    <name>javax.jdo.option.ConnectionUserName</name>
    <value>hive</value>
   
    4.javax.jdo.option.ConnectionPassword，将对应的value修改为MySQL数据库的登录密码：
    <name>javax.jdo.option.ConnectionPassword</name>
    <value>*******</value>
   
   #+END_SRC

 3. 替换${system 等值
         
    ${system:user.name}都替换为manue1
    
    ${system:java.io.tmpdir}替换为hive的临时目录 /home/manue1/opt/apache-hive-2.3.2-bin/iotmp,先创建，再替换

    : [manue1@master apache-hive-2.3.2-bin]$ mkdir iotmp
    : [manue1@master apache-hive-2.3.2-bin]$ sudo chmod -R 777 iotmp
    
    : :%s/${system:java.io.tmpdir}/\/home\/manue1\/opt\/apache-hive-2.3.2-bin\/iotmp/gg
    : :%s/${system:user.name}/manue1/gg

最后初始化metadata表数据

: schematool -initSchema -dbType mysql

** start hive

Hive的三种启动方式

1. hive  命令行模式

   进入hive安装目录，输入bin/hive的执行程序，或者输入 hive --service cli

   用于linux平台命令行查询，查询语句基本跟mysql查询语句类似

2. hive  web界面的启动方式

   bin/hive --service hwi 

   用于通过浏览器来访问hive，感觉没多大用途，浏览器访问地址是：127.0.0.1:9999/hwi

3. hive  远程服务 (端口号10000) 启动方式

   bin/hive --service hiveserver2 &

   用java，python等程序实现通过jdbc等驱动的访问hive就用这种起动方式了，这个是程序员最需要的方式了


   此时可以使用beeline 测试jdbc连接

   : beeline -u jdbc:hive2://slave01:10000 -n manue1 -p mmanue1


问题一:
#+BEGIN_SRC 

Connecting to jdbc:hive2://master:10000/default
18/01/10 20:37:17 [main]: WARN jdbc.HiveConnection: Failed to connect to master:10000
Error: Could not open client transport with JDBC Uri: jdbc:hive2://master:10000/default: Failed to open new session: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.authorize.AuthorizationException): User: manue1 is not allowed to impersonate anonymous (state=08S01,code=0)
Beeline version 2.3.2 by Apache Hive
beeline> 

分析 ： 访问权限问题

解决 ：在hdfs 的配置文件core-site.xml中加入如下配置，root为位置填入  User:*  ，etc   hadoop.proxyuser.eamon.hosts

 <property>
   <name>hadoop.proxyuser.manue1.hosts</name>
   <value>*</value>
 </property>
 <property>
  <name>hadoop.proxyuser.manue1.groups</name>
  <value>*</value>
</property>
#+END_SRC

问题二:
#+BEGIN_SRC 
ERROR 1045 (28000): Access denied for user 'hive'@'slave01' (using password: YES)

查看mysql.user表已经存在hive@%，但依然不能访问slave01,最终无解只能添加下面一条

grant all privileges on hive.* to hive@'%'identified by 'manue1';
flush privileges; 

#+END_SRC
* sqoop

 1. 环境配置

    从[[https://sqoop.apache.org][官网下载]] 解压安装，配置SQOOP_HOME目录
    #+BEGIN_SRC 
export SQOOP_HOME=/home/manue1/opt/sqoop-1.4.7.bin__hadoop-2.6.0
export PATH=$SQOOP_HOME/bin:$PATH
    #+END_SRC
    拷贝${SQOOP_HOME}/conf/sqoop-env-template.sh  
    到${SQOOP_HOME}/conf/sqoop-env.sh，
    然后修改sqoop-env.sh
    #+BEGIN_SRC 
export HADOOP_COMMON_HOME=/home/manue1/opt/hadoop-2.7.5
export HADOOP_MAPRED_HOME=/home/manue1/opt/hadoop-2.7.5
export HIVE_HOME=/home/manue1/opt/apache-hive-2.3.2-bin
    #+END_SRC

 2. 测试连接mysql
    
    将连接mysql的jar导入sqoop/lib内
    
    : sqoop list-databases --connect jdbc:mysql://slave01:3306/hive --username root --password manue1
* zookeeper
** Replicated ZooKeeper configural
   #+BEGIN_SRC 
note

For replicated mode, a minimum of three servers are required, and it is strongly recommended that you have an odd number of servers. If you only have two servers, then you are in a situation where if one of them fails, there are not enough machines to form a majority quorum. Two servers is inherently less stable than a single server, because there are two single points of failure.
     
  至少3节点,每个zookeeper服务都可以成为leader， follower，observer。
     #+END_SRC
   
   - vi conf/zoo.cfg

     sudo mkdir -p /var/lib/zookeeper
     sudo chown manue1:manue1  /var/lib/zookeeper/ #manue1 user start service
     创建 vi /var/lib/zookeeper/myid 内容为node server.x ,如 master为1
     
     #+BEGIN_SRC 

tickTime=2000
dataDir=/var/lib/zookeeper
clientPort=2181
initLimit=5
syncLimit=2
server.1=master:2888:3888   #2888 集群互相通信 3888 leader选举
server.2=slave01:2888:3888
server.3=slave02:2888:3888

# ，端口 2181 由 ZooKeeper 客户端使用，用于连接到 ZooKeeper 服务器；端口 2888 由对等 ZooKeeper 服务器使用，用于互相通信；而端口 3888 用于领导者选举

     #+END_SRC
     scp -r zookeeper-3.4.10 manue1@slave01:/home/manue1/opt
     scp -r zookeeper-3.4.10 manue1@slave02:/home/manue1/opt

   - start service
     
     nohup /home/manue1/opt/zookeeper-3.4.10/bin/zkServer.sh restart

     ps -ef | grep zookeeper

     netstat -tnlpa | grep 2181 

     echo state | nc localhost 2181
* hbase
  start hadoop & zookeeper
** hbase configural
   - .bashrc

     #+BEGIN_SRC 
# Hbase Environment Variables
export HBASE_HOME=/home/manue1/opt/hbase-1.2.6/
PATH=$JAVA_HOME/bin:$HADOOP_HOME/bin:$HBASE_HOME/bin:$PATH


     
     #+END_SRC
     
   - hbase-env.sh
     #+BEGIN_SRC 
export JAVA_HOME=/home/manue1/opt/jdk8
export HBASE_MANAGES_ZK=false
     
     #+END_SRC

   - hbase-site.xml
     
   - regionservers
     #+BEGIN_SRC 
master
slave01
slave02
     #+END_SRC

     
** hbase start

 nohup /home/manue1/opt/hbase-1.2.6/bin/start-hbase.sh &

 nohup /home/manue1/opt/hbase-1.2.6/bin/stop-hbase.sh &


 http://master:16010/master-status
* kafka
** kafka configural
   
   - conf/server.properties

     #+BEGIN_SRC 
#指定zookeeper的连接信息
zookeeper.connect=master:2181,slave01:2181,slave02:2181

#每个broker相当于一个节点，注意各个节点的broker.id的值必须唯一
broker.id=0

#broker监听端口
listeners=PLAINTEXT://master:9092

log.dir=/var/log/kafka
     #+END_SRC
      sudo mkdir -p /var/log/kafka  
      sudo chown manue1:manue1 /var/log/kafka

     scp -r kafka_2.11-1.0.0/ manue1@slave01:/home/manue1/opt
     scp -r kafka_2.11-1.0.0/ manue1@slave02:/home/manue1/opt
     各个节点修改broker.id 和 listeners

** kafka start service


   - vi .bashrc
     #+BEGIN_SRC 
# KAFKA_HOME Environment Variables
export KAFKA_HOME=/home/manue1/opt/kafka_2.11-1.0.0
export PATH=$PATH:$KAFKA_HOME/bin
     
     #+END_SRC


   nohup  $KAFKA_HOME/bin/kafka-server-start.sh $KAFKA_HOME/config/server.properties &

   $KAFKA_HOME/bin/kafka-server-stop.sh
   #+BEGIN_SRC 

   虚拟机环境内存不够，配置启动脚本 jvm heap 大小 

[manue1@slave01 config]$ $KAFKA_HOME/bin/kafka-server-start.sh $KAFKA_HOME/config/server.propertie
Java HotSpot(TM) 64-Bit Server VM warning: INFO: os::commit_memory(0x00000000c0000000, 1073741824, 0) failed; error='Cannot allocate memory' (errno=12)
   

vi kafka-server-start.sh

KAFKA_HEAP_OPTS="-Xmx256M -Xms128M
   #+END_SRC

   sudo netstat -anp | grep 9092
** use kafka

   http://blog.csdn.net/u010297957/article/details/72758765
   
   producer & consumer
   #+BEGIN_SRC 

[manue1@master ~]$ $KAFKA_HOME/bin/kafka-topics.sh --create --topic TestTopic001 --partitions 2 --replication-factor 1 --zookeeper master:2181,slave01:2181,slave02:2181
Created topic "TestTopic001".

[manue1@master ~]$ $KAFKA_HOME/bin/kafka-topics.sh --describe --topic TestTopic001 --zookeeper master:2181,slave01:2181,slave02:2181
Topic:TestTopic001	PartitionCount:2	ReplicationFactor:1	Configs:
	Topic: TestTopic001	Partition: 0	Leader: 1	Replicas: 1	Isr: 1
	Topic: TestTopic001	Partition: 1	Leader: 2	Replicas: 2	Isr: 2

[manue1@master logs]$ $KAFKA_HOME/bin/kafka-console-producer.sh --broker-list master:9092,slave01:9092,slave02:9092 --topic TestTopic001
>hi zbr
>what is your name
>i love zbr


[manue1@slave01 kafka_2.11-1.0.0]$ $KAFKA_HOME/bin/kafka-console-consumer.sh --from-beginning --topic TestTopic001 --zookeeper master:2181,slave01:2181,slave02:2181
Using the ConsoleConsumer with old consumer is deprecated and will be removed in a future major release. Consider using the new consumer by passing [bootstrap-server] instead of [zookeeper].
hi zbr
what is your name
i love zbr
   
   #+END_SRC
* spark
** spark configural

  https://data-flair.training/blogs/install-apache-spark-multi-node-cluster/


  - .bashrc
    #+BEGIN_SRC 
#SPARK_HOME Environment Variables
export SPARK_HOME=/home/manue1/opt/spark-2.2.1-bin-hadoop2.7
export PATH=$PATH:$SPARK_HOME/bin
    #+END_SRC

  - spark-env.sh
    
    cp spark-env.sh.template spark-env.sh

    #+BEGIN_SRC 
export JAVA_HOME=/home/manue1/opt/jdk8

export SCALA_HOME=/home/manue1/opt/scala-2.11.7

export HADOOP_HOME=/home/manue1/opt/hadoop-2.7.5

export HADOOP_CONF_DIR=/home/manue1/opt/hadoop-2.7.5/etc/hadoop

export SPARK_MASTER_IP=master

export SPARK_WORKER_MEMORY=64m

export SPARK_WORKER_CORES=1

export SPARK_WORKER_INSTANCES=2

变量说明
JAVA_HOME：Java安装目录
SCALA_HOME：Scala安装目录
HADOOP_HOME：hadoop安装目录
HADOOP_CONF_DIR：hadoop集群的配置文件的目录
SPARK_MASTER_IP：spark集群的Master节点的ip地址
SPARK_WORKER_MEMORY：每个worker节点能够最大分配给exectors的内存大小
SPARK_WORKER_CORES：每个worker节点所占有的CPU核数目
SPARK_WORKER_INSTANCES：每台机器上开启的worker节点的数目
    
    #+END_SRC

  - slaves
    #+BEGIN_SRC 
master
slave01
slave02
    #+END_SRC


  slave sync

  scp -r spark-2.2.1-bin-hadoop2.7/ manue1@192.168.1.109:/home/manue1/opt
** spark start

   nohup sh /home/manue1/opt/hadoop-2.7.5/sbin/start-all.sh &  #启动hdfs即可

   $SPARK_HOME/sbin/start-all.sh


   WebUI
   http://master:8080/

   spark-shell.sh 运行后可以访问后台执行的任务
   http://master:4040/

   spark-submit yarn 管理方式
   http://master:8088/
   
