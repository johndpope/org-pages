<!DOCTYPE html>
<html lang="zh-CN">
<head>
<!-- 2019-01-24 Thu 21:14 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>note-linux-centos</title>
<meta name="generator" content="Org mode">
<meta name="author" content="manue1">
<link rel="stylesheet" type="text/css" href="css/worg.css" />
<!-- <link rel="shortcut icon" href="http://www.langdebuqing.com/images/favicon.ico" type="image/x-icon" /> -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="preamble" class="status">
<div id="navbar">
    <ul>
        <li id="site-master"><a href="https://www.manue1.site/">Manue1's Journal</a></li>
        <li><a class="navbar-item" href="https://www.manue1.site/write.html">All Posts</a> </li>
        <li><a class="navbar-item" href="https://www.manue1.site/link.html">Links</a> </li>
        <li class="search">
            <form action="http://google.com/search" method="get" accept-charset="utf-8">
                <input type="search" id="search" name="q" autocomplete="off" maxlength="30" placeholder="Search..">
                <input type="hidden" name="q" value="site:www.manue1.com">
            </form>
        </li>
    </ul>
</div>
</div>
<div id="content">
<h1 class="title">note-linux-centos</h1>
<div id="table-of-contents">
<h2>&#30446;&#24405;</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orgb4bc295">1. VboxManage</a></li>
<li><a href="#org8fa9d05">2. centos7 basic environment</a>
<ul>
<li><a href="#org1db0c76">2.1. lvm</a></li>
<li><a href="#org3f2cec8">2.2. ip</a></li>
<li><a href="#orgcc029ca">2.3. sshd</a></li>
<li><a href="#orge62db2d">2.4. hostname</a></li>
<li><a href="#org2eb08b7">2.5. yum source</a></li>
<li><a href="#orgc8069de">2.6. ntp</a></li>
<li><a href="#orga1e5a5a">2.7. firewallds</a></li>
<li><a href="#org5150738">2.8. disable selinux</a></li>
<li><a href="#orgdace1ef">2.9. java  &amp; scala</a></li>
</ul>
</li>
<li><a href="#orga450f1c">3. hadoop 集群配置</a>
<ul>
<li><a href="#org3502b74">3.1. hadoop hbase spark 版本选择</a></li>
<li><a href="#org3769c7a">3.2. 环境准备</a></li>
<li><a href="#org155a3aa">3.3. 配置hadoop cluster</a></li>
<li><a href="#orgbe1e46c">3.4. 启动hadoop</a></li>
</ul>
</li>
<li><a href="#orge9a6482">4. elasticstk 集群</a>
<ul>
<li><a href="#orgc3bf245">4.1. elasticsearch-611</a></li>
<li><a href="#orga40b31e">4.2. kibana</a></li>
<li><a href="#orgab74ef3">4.3. logstash</a></li>
<li><a href="#org68cd599">4.4. beats</a>
<ul>
<li><a href="#org0633916">4.4.1. topbeat</a></li>
<li><a href="#orgecb9c41">4.4.2. filebeat</a></li>
<li><a href="#orgfcabc6e">4.4.3. metricbeat</a></li>
<li><a href="#org425ef76">4.4.4. packetbeat</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org8502d96">5. hive</a>
<ul>
<li><a href="#org51c3ed6">5.1. install mariadb</a></li>
<li><a href="#org37af443">5.2. config hive</a></li>
<li><a href="#orgc20d3d0">5.3. start hive</a></li>
</ul>
</li>
<li><a href="#org87cae1b">6. sqoop</a></li>
<li><a href="#org9db1495">7. zookeeper</a>
<ul>
<li><a href="#org4d0af91">7.1. Replicated ZooKeeper configural</a></li>
</ul>
</li>
<li><a href="#org29c71df">8. hbase</a>
<ul>
<li><a href="#org72db9d3">8.1. hbase configural</a></li>
<li><a href="#orge6e4751">8.2. hbase start</a></li>
</ul>
</li>
<li><a href="#org9d606ad">9. kafka</a>
<ul>
<li><a href="#orgbd36419">9.1. kafka configural</a></li>
<li><a href="#org539271a">9.2. kafka start service</a></li>
<li><a href="#org8634632">9.3. use kafka</a></li>
</ul>
</li>
<li><a href="#org6494061">10. spark</a>
<ul>
<li><a href="#org6fd3e2b">10.1. spark configural</a></li>
<li><a href="#org8161813">10.2. spark start</a></li>
</ul>
</li>
</ul>
</div>
</div>
<p>
vbox自建centos集群环境，方便本地开发测试,<br>
</p>

<div id="outline-container-orgb4bc295" class="outline-2">
<h2 id="orgb4bc295"><span class="section-number-2">1</span> VboxManage</h2>
<div class="outline-text-2" id="text-1">
<ul class="org-ul">
<li><p>
vms 基本信息<br>
</p>

<p>
VBoxManage showvminfo Centos701<br>
</p></li>

<li><p>
无界面启动vms<br>
</p>

<p>
VBoxManage startvm Centos701 &#x2013;type headless<br>
</p>

<p>
VBoxManage controlvm ubuntu01 savestate # 保存当前状态关闭<br>
</p></li>

<li><p>
修改vms内存总量<br>
</p>

<p>
VBoxManage modifyvm Centos701 &#x2013;memory 1024<br>
</p></li>

<li><p>
修改vms磁盘容量<br>
</p>

<p>
VBoxManage modifyhd Centos702.vdi &#x2013;resize 7168<br>
</p>

<pre class="example">

如果创建过快照，快照的磁盘空间也需要修改
VBoxManage modifyhd \{aea51811-5e79-46e9-b5ba-f66740e6f47b\}.vdi  --resize 10248

</pre></li>

<li><p>
创建快照<br>
</p>

<p>
VBoxManage snapshot ubuntu01 take ubuntu_basic_ssh<br>
</p>

<p>
VBoxManage snapshot ubuntu01 list<br>
</p>

<p>
VBoxManage snapshot ubuntu01 delete ubuntu_basic<br>
</p>

<p>
VBoxManage snapshot ubuntu01 restore ubuntu_basic_ssh<br>
</p></li>

<li><p>
clone vm<br>
</p>

<p>
VBoxManage clonevm  Centos701 &#x2013;snapshot  centos7_basic  &#x2013;name Centos_basic<br>
</p></li>
</ul>

<pre class="example">

查看当前虚拟机  VBoxManage list vms  

查看当前正在运行的虚拟机  VBoxManage list runningvms  

启动虚拟机  VBoxManage startvm 虚拟机名  

无前端图形界面方式启动虚拟机  VBoxManage startvm 虚拟机名 --type headless  

</pre>
</div>
</div>

<div id="outline-container-org8fa9d05" class="outline-2">
<h2 id="org8fa9d05"><span class="section-number-2">2</span> centos7 basic environment</h2>
<div class="outline-text-2" id="text-2">
<p>
centos7 镜像<a href="http://isoredirect.centos.org/centos/7/isos/x86_64/CentOS-7-x86_64-Minimal-1708.iso">下载</a><br>
</p>
</div>

<div id="outline-container-org1db0c76" class="outline-3">
<h3 id="org1db0c76"><span class="section-number-3">2.1</span> lvm</h3>
<div class="outline-text-3" id="text-2-1">
<p>
在文件中添加要挂载的分区和文件目录可以修改文件<br>
</p>

<p>
/etc/fstab<br>
</p>

<p>
<i>dev/sda5</i>    media/win    ntfs    defaults   02<br>
</p>

<p>
然后 mount -a<br>
</p>

<ol class="org-ol">
<li><p>
查看几块硬盘<br>
</p>

<p>
sudo fdisk -l |grep sd<br>
</p></li>

<li><p>
创建分区<br>
</p>

<p>
虚拟机现有20g的硬盘,使用fdisk划分磁盘<br>
</p>

<p>
sudo fdisk /dev/sda<br>
</p>

<p class="verse">
m  帮助信息<br>
n 创建分区<br>
e 扩展分区    +5G  pppp/pppe<br>
p 打印分区<br>
t 分区类型 L  (lvm)<br>
w 写入保存分区<br>
</p></li>

<li>格式化 分区<br></li>

<li><p>
LVM<br>
pv &#x2013;&gt; vg &#x2013;&gt; lv<br>
参考:<br>
<a href="http://blog.sina.com.cn/s/blog_b77735d20101e5cn.html">http://blog.sina.com.cn/s/blog_b77735d20101e5cn.html</a><br>
<a href="http://aurthurxlc.github.io/Aurthur-2017/Centos-7-extend-lvm-volume.html">http://aurthurxlc.github.io/Aurthur-2017/Centos-7-extend-lvm-volume.html</a><br>
</p>

<pre class="example">
fdisk -l | grep sd
fdisk /dev/sda
partprobe
pvdisplay
pvcreate /dev/sda3
vgdisplay
vgextend centos /dev/sda3
lvdisplay
#lvcreate -L 3.31G -n manue1 centos
#mkfs.xfs /dev/centos/manue1
#lvremove -f /dev/centos/manue1
lvextend -l +100%FREE /dev/centos/root
df -Th
xfs_growfs /dev/centos/root
</pre></li>
</ol>
</div>
</div>
<div id="outline-container-org3f2cec8" class="outline-3">
<h3 id="org3f2cec8"><span class="section-number-3">2.2</span> ip</h3>
<div class="outline-text-3" id="text-2-2">
<ul class="org-ul">
<li><p>
联网方式: 配置三张网卡<br>
</p>

<ol class="org-ol">
<li>NAT<br>
网卡1 用来连接外网<br></li>
<li><p>
Host-only<br>
用来配置静态IP 配置集群服务的时候不需要修改IP<br>
vi <i>etc/sysconfig/network-scripts</i>  <br>
</p>
<pre class="example">
#static assignment
ONBOOT=yes
BOOTPROTO=static
IPADDR=192.168.56.10
NETMASK=255.255.255.0
GATEWAY=192.168.56.1
</pre></li>
<li>Bridge<br>
vbox 自动配置IP，也很方便<br></li>
</ol>

<p>
这边打算使用网卡1 nat模式连接外网，网卡3的桥接模式与局域网内其他主机通信,网卡二的主机模式搭建集群<br>
</p>

<p>
注意： 网卡二和网卡三的 gateway 字段要注释掉<br>
</p>

<p>
sudo service network restart<br>
</p></li>
</ul>
</div>
</div>

<div id="outline-container-orgcc029ca" class="outline-3">
<h3 id="orgcc029ca"><span class="section-number-3">2.3</span> sshd</h3>
<div class="outline-text-3" id="text-2-3">
<p>
ssh 连接异常慢<br>
</p>

<p>
sudo vi /etc/ssh/sshd_config<br>
</p>

<pre class="example">
UseDNS no

</pre>
</div>
</div>

<div id="outline-container-orge62db2d" class="outline-3">
<h3 id="orge62db2d"><span class="section-number-3">2.4</span> hostname</h3>
<div class="outline-text-3" id="text-2-4">
<p>
永久修改主机名字<br>
</p>

<p>
sudo hostnamectl &#x2013;static set-hostname master<br>
</p>

<p>
sudo vi /etc/hosts<br>
</p>

<pre class="example">
[manue1@localhost ~]$ cat /etc/hostname
 master
[manue1@localhost ~]$ cat /etc/hosts
 127.0.0.1 master
 ::1 master
</pre>
</div>
</div>


<div id="outline-container-org2eb08b7" class="outline-3">
<h3 id="org2eb08b7"><span class="section-number-3">2.5</span> yum source</h3>
<div class="outline-text-3" id="text-2-5">
<p>
sudo yum -y install wget<br>
</p>

<ul class="org-ul">
<li><p>
备份<br>
</p>

<p>
sudo mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup<br>
</p></li>

<li><p>
设置aliyun source<br>
</p>

<p>
sudo wget -O /etc/yum.repos.d/CentOS-Base.repo <a href="http://mirrors.aliyun.com/repo/Centos-7.repo">http://mirrors.aliyun.com/repo/Centos-7.repo</a><br>
</p></li>

<li><p>
设置EPLEPEL source<br>
</p>

<p>
sudo wget -P <i>etc/yum.repos.d</i> <a href="http://mirrors.aliyun.com/repo/epel-7.repo">http://mirrors.aliyun.com/repo/epel-7.repo</a><br>
</p>

<p>
添加后可以像fedora上 yum install packname<br>
</p></li>

<li><p>
清理缓存并生成新的缓存<br>
</p>

<p>
sudo yum clean all<br>
</p>

<p>
sudo yum makecache<br>
</p></li>
</ul>
</div>
</div>

<div id="outline-container-orgc8069de" class="outline-3">
<h3 id="orgc8069de"><span class="section-number-3">2.6</span> ntp</h3>
<div class="outline-text-3" id="text-2-6">
<table>


<colgroup>
<col  class="org-left">

<col  class="org-left">
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">master</th>
<th scope="col" class="org-left">server</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">slave01</td>
<td class="org-left">client</td>
</tr>

<tr>
<td class="org-left">slave02</td>
<td class="org-left">client</td>
</tr>
</tbody>
</table>

<ul class="org-ul">
<li><p>
any nodes<br>
</p>

<p>
sudo yum -y install ntp<br>
</p>

<p>
timedatectl set-timezone Asia/Shanghai   # 设置上海时区<br>
</p></li>

<li><p>
server configural<br>
</p>

<p>
systemctl start ntpd<br>
</p>

<p>
systemctl enable ntpd<br>
</p>

<p>
vi /etc/ntp.conf<br>
</p>

<pre class="example">

restrict 192.168.56.0 mask 255.255.0.0

server 127.127.1.0

fudge 127.127.1.0 stratum 10
</pre>

<p>
systemctl restart ntpd<br>
</p>

<ul class="org-ul">
<li><p>
client configural<br>
</p>

<p>
systemctl start ntpd<br>
</p>

<p>
systemctl enable ntpd<br>
</p>

<p>
vi /etc/ntp.conf<br>
</p>

<pre class="example">
server 192.168.56.10

</pre>

<p>
netstat -anp | grep 123<br>
</p></li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orga1e5a5a" class="outline-3">
<h3 id="orga1e5a5a"><span class="section-number-3">2.7</span> firewallds</h3>
<div class="outline-text-3" id="text-2-7">
<dl class="org-dl">
<dt>查看状态</dt><dd>systemctl status firewalld<br></dd>
<dt>关闭</dt><dd>systemctl stop firewalld<br></dd>
<dt>禁用</dt><dd>systemctl disable firewalld<br></dd>
</dl>
</div>
</div>
<div id="outline-container-org5150738" class="outline-3">
<h3 id="org5150738"><span class="section-number-3">2.8</span> disable selinux</h3>
<div class="outline-text-3" id="text-2-8">
<p>
一款为了提高系统安全性的软件：对系统服务，文件权限，网络端口访问有极其严格的限制，<br>
例如：如果对一个文件没有正确安全上下文配置， 甚至你是root用户，你也不能启动某服务<br>
</p>

<p>
sudo vi /etc/sysconfig/selinux<br>
</p>
<pre class="example">
selinux = disable

</pre>
</div>
</div>
<div id="outline-container-orgdace1ef" class="outline-3">
<h3 id="orgdace1ef"><span class="section-number-3">2.9</span> java  &amp; scala</h3>
<div class="outline-text-3" id="text-2-9">
<p>
基础环境用root 配置在/etc/profile 自启动环境文件内<br>
</p>

<p>
refer : <a href="https://www.mtyun.com/library/how-to-setup-scala-on-centos7">1</a><br>
</p>

<ul class="org-ul">
<li>java rpm install<br>

<ol class="org-ol">
<li><a href="http://www.oracle.com/technetwork/java/javase/downloads/index.html">download</a><br></li>
<li><p>
install<br>
</p>
<p class="verse">
sudo rpm -ivh jdk-8u144-linux-x64.rpm<br>
sudo rpm -aq | grep jdk<br>
sudo rpm -e jdk   无效<br>
sudo yum remove jdk<br>
</p>

<p>
sudo vi /etc/profile<br>
</p>

<pre class="example">
#JAVA_HOME
export JAVA_HOME=/usr/java/jdk1.8.0_144
export CLASSPATH=.:$JAVA_HOME/jre/lib/rt.jar:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
export PATH=$PATH:$JAVA_HOME/bin
</pre></li>
</ol></li>
<li><p>
java 离线包安装<br>
</p>

<p>
tar -zxvf jdk-8u151-linux-x64.tar.gz<br>
</p>

<p>
vi /etc/profile<br>
</p>
<pre class="example">
#JAVA_HOME
JAVA_HOME=/home/manue1/opt/jdk8
PATH=$PATH:$JAVA_HOME/bin
export JAVA_HOME PATH
</pre></li>
<li><p>
scala 离线包安装<br>
</p>

<p>
当前最新版本<br>
</p>

<p>
tar zxvf scala-2.11.7.tgz<br>
</p>

<pre class="example">
SCALA_HOME=/home/manue1/opt/scala-2.11.7
PATH=$PATH:$SCALA_HOME/bin
export SCALA_HOME PATH
</pre></li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orga450f1c" class="outline-2">
<h2 id="orga450f1c"><span class="section-number-2">3</span> hadoop 集群配置</h2>
<div class="outline-text-2" id="text-3">
</div>
<div id="outline-container-org3502b74" class="outline-3">
<h3 id="org3502b74"><span class="section-number-3">3.1</span> hadoop hbase spark 版本选择</h3>
<div class="outline-text-3" id="text-3-1">
<ul class="org-ul">
<li><p>
hbase 支持 hadoop 版本对照表<br>
</p>

<p>
The 1.2.x series is the current stable release line<br>
</p>

<p>
<a href="http://www-us.apache.org/dist/hbase/">http://www-us.apache.org/dist/hbase/</a><br>
</p>

<p>
下面查看1.2.x 需要的hadoop版本<br>
</p>

<p>
<a href="http://hbase.apache.org/book.html#arch.overview">http://hbase.apache.org/book.html#arch.overview</a><br>
</p>

<p>
crtl + F  "s" 搜索页面<br>
</p>

<p>
选择 Hadoop-2.7.1+<br>
</p></li>

<li><p>
spark 支持 hadoop<br>
</p>

<p>
<a href="http://spark.apache.org/downloads.html">http://spark.apache.org/downloads.html</a><br>
</p>

<p>
官方下载页面可以手动选择<br>
</p></li>

<li><p>
hive 支持 hadoop<br>
</p>

<p>
<a href="https://hive.apache.org/downloads.html">https://hive.apache.org/downloads.html</a><br>
</p>

<p>
稳定版下载地址<br>
</p>

<p>
<a href="http://mirrors.shuosc.org/apache/hive/stable-2/">http://mirrors.shuosc.org/apache/hive/stable-2/</a><br>
</p></li>

<li><p>
zookeeper<br>
</p>

<p>
下载稳定版即可<br>
</p>

<p>
<a href="http://mirrors.shuosc.org/apache/zookeeper/stable/">http://mirrors.shuosc.org/apache/zookeeper/stable/</a><br>
</p></li>
</ul>
</div>
</div>

<div id="outline-container-org3769c7a" class="outline-3">
<h3 id="org3769c7a"><span class="section-number-3">3.2</span> 环境准备</h3>
<div class="outline-text-3" id="text-3-2">
<p>
三台vbox 虚拟centos7 配置 java scala 环境 关闭防火墙和selinux<br>
</p>

<ul class="org-ul">
<li><p>
cluster<br>
</p>
<table>


<colgroup>
<col  class="org-left">

<col  class="org-right">
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">hostname</th>
<th scope="col" class="org-right">ip</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">master</td>
<td class="org-right">192.168.56.10</td>
</tr>
</tbody>
<tbody>
<tr>
<td class="org-left">slave01</td>
<td class="org-right">192.168.56.11</td>
</tr>
</tbody>
<tbody>
<tr>
<td class="org-left">slave02</td>
<td class="org-right">192.168.56.12</td>
</tr>
</tbody>
</table></li>

<li><p>
disable ipv6<br>
</p>

<p>
sudo vi /etc/sysctl.conf<br>
</p>

<p>
添加下面内容<br>
</p>

<pre class="example">

# disable ipv6
net.ipv6.conf.all.disable_ipv6 = 1
net.ipv6.conf.default.disable_ipv6 = 1
net.ipv6.conf.lo.disable_ipv6 = 1

</pre>

<p>
解决master:50070 页面找不到live node<br>
</p>

<p>
解决 connection exception<br>
</p>

<pre class="example">
17//23 23:19:48 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
ls:all From slave01/127.0.0.1 to master:9000 failed on connection exception: java.net.ConnectException: 拒绝连接; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused

</pre></li>

<li><p>
hostname &amp; host<br>
</p>

<p>
三台主机都要,修改主机名,修改/etc/hosts 互相添加hostname访问别名<br>
</p>

<p>
注意； #127.0.0.1 master 这样的映射一定要注释掉,master:8088无法访问最终定位到这里了<br>
</p>
<pre class="example">
#ceos7 cluster
19268.56.10 master
19268.56.11 slave01
19268.56.12 slave02
</pre></li>

<li><p>
免登录验证<br>
</p>
<p class="verse">
ssh-keygen -t rsa<br>
ssh-copy-id -i ~/.ssh/id_rsa.pub manue1@slave01<br>
ssh-copy-id -i ~/.ssh/id_rsa.pub manue1@slave02<br>
ssh-copy-id -i ~/.ssh/id_rsa.pub manue1@master<br>
</p></li>

<li>download hadoop<br>
tar -zxvf hadoop-2.7.5.tar.gz<br></li>
</ul>
</div>
</div>
<div id="outline-container-org155a3aa" class="outline-3">
<h3 id="org155a3aa"><span class="section-number-3">3.3</span> 配置hadoop cluster</h3>
<div class="outline-text-3" id="text-3-3">
<ul class="org-ul">
<li><p>
hadoop_home<br>
三台节点都需要配置<br>
</p>

<p>
vi .bashrc<br>
</p>
<pre class="example">
# Hadoop Environment Variables
export HADOOP_HOME=/home/manue1/opt/hadoop-2.7.5
export HADOOP_INSTALL=$HADOOP_HOME
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin
export HADOOP_OPTS="$HADOOP_OPTS -Djava.library.path=$HADOOP_HOME/lib/native" #解决WARN util.NativeCodeLoader: Unable to load native-hadoop library
export CATALINA_BASE=$HADOOP_HOME/share/hadoop/httpfs/tomcat #支持httpfs rest api

</pre></li>

<li><p>
master<br>
</p>

<p>
<i>home/manue1/opt/hadoop-2.7.5/etc/hadoop</i> 下6个配置文件<br>
</p>

<ol class="org-ol">
<li><p>
core-site.xml<br>
</p>

<pre class="example">
&lt;configuration&gt;
    &lt;!-- 指定HDFS老大（namenode）的通信地址 --&gt;
    &lt;property&gt;
        &lt;name&gt;fs.defaultFS&lt;/name&gt;
        &lt;value&gt;hdfs://master:9000&lt;/value&gt;
    &lt;/property&gt;
    &lt;!-- 指定hadoop运行时产生文件的存储路径 --&gt;
    &lt;property&gt;
        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
        &lt;value&gt;file:/home/manue1/opt/hadoop-2.7.5/tmp&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;
    &lt;!--开启httpfs实现一种匿名的方式登陆hdfs文件系统 端口14000
        manue1用户为hdfs的超级用户 hive启动用户
    --&gt;

    &lt;property&gt;
         &lt;name&gt;hadoop.proxyuser.manue1.hosts&lt;/name&gt;
         &lt;value&gt;*&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;hadoop.proxyuser.manue1.groups&lt;/name&gt;
        &lt;value&gt;*&lt;/value&gt;
    &lt;/property&gt;


</pre></li>

<li><p>
hdfs-site.xml<br>
</p>

<pre class="example">
&lt;configuration&gt;
        &lt;!-- 设置namenode的http通讯地址 --&gt;
        &lt;property&gt;
                &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;
                &lt;value&gt;master:50090&lt;/value&gt;
        &lt;/property&gt;
        &lt;!-- 设置hdfs副本数量 --&gt;
        &lt;property&gt;
                &lt;name&gt;dfs.replication&lt;/name&gt;
                &lt;value&gt;1&lt;/value&gt;
        &lt;/property&gt;
         &lt;!-- 设置namenode存放的路径 --&gt;
        &lt;property&gt;
                &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;
                &lt;value&gt;file:/home/manue1/opt/hadoop-2.7.5/tmp/dfs/name&lt;/value&gt;
        &lt;/property&gt;
         &lt;!-- 设置datanode存放的路径 --&gt;
        &lt;property&gt;
                &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
                &lt;value&gt;file:/home/manue1/opt/hadoop-2.7.5/tmp/dfs/data&lt;/value&gt;
        &lt;/property&gt;
&lt;/configuration&gt;


</pre></li>

<li><p>
mapred-site.xml<br>
</p>

<p>
mv mapred-site.xml.template mapred-site.xml<br>
</p>

<pre class="example">
&lt;configuration&gt;
        &lt;!-- 通知框架MR使用YARN --&gt;
        &lt;property&gt;
                &lt;name&gt;mapreduce.framework.name&lt;/name&gt;
                &lt;value&gt;yarn&lt;/value&gt;
        &lt;/property&gt;
        &lt;property&gt;
                &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;
                &lt;value&gt;master:10020&lt;/value&gt;
        &lt;/property&gt;
        &lt;property&gt;
                &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;
                &lt;value&gt;master:19888&lt;/value&gt;
        &lt;/property&gt;
&lt;/configuration&gt;

</pre></li>

<li><p>
yarn-site.xml<br>
</p>

<pre class="example">
&lt;configuration&gt;
 &lt;!-- 设置 resourcemanager 在哪个节点--&gt;
&lt;!-- Site specific YARN configuration properties --&gt;
        &lt;property&gt;
                &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;
                &lt;value&gt;master&lt;/value&gt;
        &lt;/property&gt;
         &lt;!-- reducer取数据的方式是mapreduce_shuffle --&gt;
        &lt;property&gt;
                &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
                &lt;value&gt;mapreduce_shuffle&lt;/value&gt;
        &lt;/property&gt;
        &lt;!--所有主机访问yarn管理界面--&gt;
        &lt;property&gt; 
                &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt;
                &lt;value&gt;0.0.0.0:8088&lt;/value&gt;
        &lt;/property&gt;

&lt;/configuration&gt;

</pre></li>

<li><p>
slaves<br>
</p>

<pre class="example">
slave01
slave02
</pre></li>

<li><p>
hadoop-env.sh<br>
</p>

<p>
修改<br>
export JAVA_HOME=/home/manue1/opt/jdk8<br>
</p></li>
</ol></li>

<li><p>
slaves<br>
</p>

<p>
复制master节点配置好的安装包到指定slaves目录<br>
</p>

<pre class="example">
tar -zcvf hadoop-2.7.5_conf_finshed.tar.gz hadoop-2.7.5/
scp hadoop-2.7.5_conf_finshed.tar.gz manue1@slave02:/home/manue1/opt/

</pre></li>
</ul>
</div>
</div>

<div id="outline-container-orgbe1e46c" class="outline-3">
<h3 id="orgbe1e46c"><span class="section-number-3">3.4</span> 启动hadoop</h3>
<div class="outline-text-3" id="text-3-4">
<p>
第一次启动要执行格式化，之后启动不用执行这条<br>
</p>
<pre class="example">
hdfs namenode -format 

</pre>

<p>
启动命令:<br>
</p>
<pre class="example">
start-dfs.sh
start-yarn.sh
mr-jobhistory-daemon.sh start historyserver  ??
httpfs.sh start

</pre>


<ul class="org-ul">
<li><p>
master<br>
</p>
<pre class="example">
manue1@master sbin]$ jps
2034 NameNode
2483 Jps
15754 Bootstrap  #httpfs
1652 ResourceManager
2188 SecondaryNameNode
2447 JobHistoryServer

</pre></li>

<li><p>
slaves<br>
</p>
<pre class="example">
[manue1@slave01 hadoop]$ jps
1360 DataNode
1430 NodeManager
1516 Jps
</pre></li>
</ul>

<p>
hadoop cluster状态展示界面 webhdfs<br>
</p>

<pre class="example">
http://master:50070/
curl "http://master:50070/webhdfs/v1/?op=liststatus&amp;user.name=manue1"

</pre>

<p>
httpfs rest api 配置HA的时候找不到namenode可以采用httpfs<br>
</p>

<pre class="example">
http://master:14000/
curl "http://master:14000/webhdfs/v1/?op=liststatus&amp;user.name=manue1"

</pre>

<p>
yarn 管理界面<br>
</p>

<pre class="example">
http://master:8088

</pre>
</div>
</div>
</div>

<div id="outline-container-orge9a6482" class="outline-2">
<h2 id="orge9a6482"><span class="section-number-2">4</span> elasticstk 集群</h2>
<div class="outline-text-2" id="text-4">
</div>
<div id="outline-container-orgc3bf245" class="outline-3">
<h3 id="orgc3bf245"><span class="section-number-3">4.1</span> elasticsearch-611</h3>
<div class="outline-text-3" id="text-4-1">
<blockquote>
<ol class="org-ol">
<li><p>
环境准备<br>
</p>

<table>


<colgroup>
<col  class="org-left">

<col  class="org-right">

<col  class="org-left">
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">hostname</th>
<th scope="col" class="org-right">ip</th>
<th scope="col" class="org-left">&#xa0;</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">master</td>
<td class="org-right">192.168.56.10</td>
<td class="org-left">masternode</td>
</tr>
</tbody>
<tbody>
<tr>
<td class="org-left">slave01</td>
<td class="org-right">192.168.56.11</td>
<td class="org-left">datanode</td>
</tr>
</tbody>
<tbody>
<tr>
<td class="org-left">slave02</td>
<td class="org-right">192.168.56.12</td>
<td class="org-left">datanode</td>
</tr>
</tbody>
</table>

<ul class="org-ul">
<li><p>
生产环境配置:<br>
</p>

<p>
10节点 6个master候选节点<br>
</p>

<p>
1节点 4G + lucene 3G = 8G<br>
</p>

<p>
10T硬盘，最多存5T数据<br>
</p>

<p>
4c + 8g + 1T * 10台 = 10T<br>
</p>

<p>
4c + 16g + 2T * 5台 = 10T<br>
</p></li>

<li>java 环境配置，关闭firewalld,主机名配置,官网下载elasticsearch-6.1.1.tar.gz<br></li>

<li>普通用户下安装es  username:manue1<br></li>

<li><p>
系统设置<br>
</p>

<p>
sudo -s 切换到root下执行<br>
</p>

<pre class="example">

sed -e '$a vm.max_map_count = 262144' -i /etc/sysctl.conf

sysctl -p



echo "ulimit -SHn 1048576" &gt;&gt; /etc/rc.local

sed -e '$a DefaultLimitCORE=infinity\nDefaultLimitNOFILE=1048576\nDefaultLimitNPROC=1048576' -i /etc/systemd/system.conf

cat &gt;&gt; /etc/security/limits.conf &lt;&lt; EOF

 *           soft   nofile       1048576

 *           hard   nofile       1048576

 *           soft   nproc        1048576

 *           hard   nproc        1048576

EOF

sed -i 's/4096/1048576/' /etc/security/limits.d/20-nproc.conf

sed -e '/root       soft    nproc     unlimited/a\*           soft   nofile       1048576\n*           hard   nofile       1048576' -i /etc/security/limits.d/20-nproc.conf

</pre>

<p>
修改系统配置文件后，重启系统生效<br>
</p></li>
</ul></li>

<li><p>
配置elasticsearch<br>
</p>

<ul class="org-ul">
<li><p>
elasticsearch.yml          # els的配置文件<br>
</p>

<pre class="example">
cluster.name: manue1-es-cluster  #集群名称

node.name: master-node           #节点名称

node.data: false
node.master: true  #建议直接不设置，默认两个都为true.

path.data: /Home/Manue1/Opt/Elasticsearch-6.1.1/Els/Data  #数据存储目录

path.logs: /home/manue1/opt/elasticsearch-6.1.1/els/log   #日志存储目录

network.bind_host: 0.0.0.0   #master节点配置 ”0.0.0.0”，允许所有网络接口访问
network.publish_host: master # 集群通信

gateway.recover_after_nodes: 3  #值为n，网关控制在n个节点启动之后才恢复整个集群, 3节&gt;点启动后1分钟
gateway.recover_after_time: 1m

indices.recovery.max_bytes_per_sec: 20mb  #恢复数据时,限制的宽带流量,如果是0就是无限制

node.max_local_storage_nodes: 1                  #值为n，一个系统中最多启用节点个数为n

http.port: 9200                 # 对外提供服务的端口，9300为集群服务的端口


</pre></li>

<li><p>
jvm.options                # JVM相关的配置，内存大小等等<br>
</p>

<pre class="example">
-Xms128M
-Xmx128M
-Xmx1g与-Xms1gJVM的最大最小内存。如果太小会导致Elasticsearch刚刚启动就立刻停止。太大会拖慢系统本身
</pre></li>

<li>log4j2.properties          # 日志系统定义<br></li>
</ul>

<p>
将配置好的elasticsearch 打包传到各个节点，需要注意的是，如果配置过程中运行产生的data/nodes/0 文件<br>
一定要删掉，再打包使用，否则各个节点启动成功了，无法加入到集群，节点id冲突<br>
报错信息: <code>with the same id but is a different node instance</code><br>
</p></li>

<li><p>
启动elasticsearch<br>
</p>

<p>
su manue1<br>
</p>

<p>
vi  /home/manue1/opt/elasticsearch-6.1.1/bin/elasticsearch<br>
</p>

<pre class="example">
ES_HEAP_SIZE=128m
MAX_OPEN_FILES=262144
</pre>

<p>
nohup ./bin/elasticsearch -d<br>
</p>

<p>
关闭 ps -ef |grep elasticsearch|awk '{print $2}'|xargs kill -9<br>
</p></li>

<li><p>
refer<br>
</p>

<p>
<a href="https://blog.csdn.net/thomas0yang/article/details/55518105#%E5%86%85%E5%AD%98">1</a> <a href="https://www.elastic.co/guide/cn/elasticsearch/guide/cn/important-configuration-changes.html#_%E6%9C%80%E5%B0%8F%E4%B8%BB%E8%8A%82%E7%82%B9%E6%95%B0">2</a> <a href="https://zhuanlan.zhihu.com/p/35291900">3</a><br>
</p></li>
</ol>

<p>
`<br>
</p>
</blockquote>
</div>
</div>

<div id="outline-container-orga40b31e" class="outline-3">
<h3 id="orga40b31e"><span class="section-number-3">4.2</span> kibana</h3>
<div class="outline-text-3" id="text-4-2">
<p>
配置在es的非数据节点上: 192.168.56.10<br>
</p>

<p>
修改 config/kibana.yml<br>
</p>
<pre class="example">
server.host: "0.0.0.0" #不同网卡网段能够访问
elasticsearch.url: "http://master:9200"
</pre>

<p>
启动： nohup  bin/kibana  &amp;<br>
</p>

<p>
关闭: ps -ef |grep kibana |awk '{print $2}'|xargs kill -9<br>
</p>

<p>
ss -lnp | grep 5601<br>
</p>
</div>
</div>

<div id="outline-container-orgab74ef3" class="outline-3">
<h3 id="orgab74ef3"><span class="section-number-3">4.3</span> logstash</h3>
<div class="outline-text-3" id="text-4-3">
<ol class="org-ol">
<li>download<br>
logstash-6.1.1.tar.gz<br></li>
<li>config<br>

<ul class="org-ul">
<li><p>
创建logstash-conf 目录<br>
beat的配置文件<br>
vi beats.conf<br>
</p>
<pre class="example">
input {
  beats {
    port =&gt; 5044
  }
}

# The filter part of this file is commented out to indicate that it is
# optional.
# filter {
#
# }

output {
  elasticsearch {
    hosts =&gt; "master:9200"
    manage_template =&gt; false
    index =&gt; "%{[@metadata][beat]}-%{[@metadata][version]}-%{+YYYY.MM.dd}" 
  }
}

</pre></li>

<li>jvm.options<br>
修改 xms xmx 最大最小jvm 为256M 比es测试集群吃内存多<br></li>

<li>logstash.yml<br></li>
</ul></li>

<li>start logstash<br>

<ul class="org-ul">
<li>bin/logstash -e 'input { stdin { } } output { stdout {} }'<br>
测试启动<br></li>

<li><p>
./bin/logstash -f logstash-conf/beats.conf &amp;<br>
</p>

<p>
配置文件启动<br>
</p>

<p>
sudo netstat -anp | grep 5044<br>
</p></li>
</ul></li>
</ol>
</div>
</div>
<div id="outline-container-org68cd599" class="outline-3">
<h3 id="org68cd599"><span class="section-number-3">4.4</span> beats</h3>
<div class="outline-text-3" id="text-4-4">
</div>
<div id="outline-container-org0633916" class="outline-4">
<h4 id="org0633916"><span class="section-number-4">4.4.1</span> topbeat</h4>
<div class="outline-text-4" id="text-4-4-1">
<p>
5.x版本后弃用了<br>
</p>
<ol class="org-ol">
<li>下载<br>
topbeat-1.3.1-x86_64.tar.gz<br></li>

<li>配置<br>

<ul class="org-ul">
<li>topbeat<br></li>

<li>topbeat.template.json<br>
topbeat自带的模版，用来创建存放收集数据的索引结构<br></li>

<li><p>
topbeat.yml<br>
</p>
<pre class="example">
input:
  period: 10           #默认10秒收集一次
  procs: [".*"]   #定义正则表达式，以匹配你所要监控的进程。默认是所有正在运行的进程都进行监控。
  stats:
    system: true
    proc: true
    filesystem: true
output:
  elasticsearch:
    hosts: ["master:9200"]
shipper:
logging:
  files:
</pre></li>
</ul></li>
</ol>


<ol class="org-ol">
<li>es导入模版<br>
导入topbeat自带的模版，用来创建存放收集数据的索引结构<br>

<ul class="org-ul">
<li><p>
Configuring Template Loading - supported for Elasticsearch output only<br>
</p>
<pre class="example">
ERR Failed to perform any bulk index operations: 406 Not Acceptable
错误应该是模版和6.0版本不匹配了，官网没有更新
再去官网查看，topbeat 从5.0 已经被 Metricbeat替换了
</pre></li>

<li>Loading the Template Manually - required for Logstash output<br></li>
</ul></li>
</ol>


<ol class="org-ol">
<li>kibana<br></li>

<li><p>
启动topbeat节点<br>
</p>

<p>
sudo ./topbeat -e -c topbeat.yml -d "publish"<br>
</p></li>
</ol>
</div>
</div>
<div id="outline-container-orgecb9c41" class="outline-4">
<h4 id="orgecb9c41"><span class="section-number-4">4.4.2</span> filebeat</h4>
<div class="outline-text-4" id="text-4-4-2">
<ol class="org-ol">
<li><p>
download<br>
</p>

<p>
filebeat-6.1.1-linux-x86_64.tar.gz<br>
</p>

<p>
<a href="https://download.elastic.co/demos/logstash/gettingstarted/logstash-tutorial.log.gz">https://download.elastic.co/demos/logstash/gettingstarted/logstash-tutorial.log.gz</a><br>
logstash-tutorial.log.gz apache 的日志文件样本<br>
</p></li>

<li>config<br>

<ul class="org-ul">
<li><p>
filebeat.yml<br>
</p>
<pre class="example">
- type: log
  # Change to true to enable this prospector configuration.
  enabled: true
  # Paths that should be crawled and fetched. Glob based paths.
  paths:
    - /var/log/*.log
    - /home/manue1/opt/source/*.log
    #- c:\programdata\elasticsearch\logs\*


output.logstash:
  # The Logstash hosts
  hosts: ["master:5044"]


setup.kibana:

  host: "master:5601"         

</pre></li>

<li><p>
modules<br>
</p>

<p>
sudo chown -R root /home/manue1/opt/filebeat-6.1.1-linux-x86_64/module<br>
</p>

<p>
sudo chown -R root /home/manue1/opt/filebeat-6.1.1-linux-x86_64/modules.d<br>
</p>

<ul class="org-ul">
<li><p>
Enable modules when you run Filebeatedit<br>
</p>

<p>
sudo ./filebeat -e &#x2013;modules system,nginx,mysql<br>
</p>

<p>
./filebeat -e &#x2013;modules nginx -M "nginx.access.var.paths=[/var/log/nginx/access.log*]"<br>
</p></li>

<li><p>
filebeat.yml<br>
</p>

<p>
sudo ./filebeat modules list<br>
</p>

<p>
sudo ./filebeat modules enable system<br>
</p>

<pre class="example">
默认配置读取所有enable
filebeat.modules:
- module: nginx
- module: mysql
- module: system

</pre></li>
</ul></li>
</ul></li>
</ol>





<ul class="org-ul">
<li><p>
setup template<br>
</p>

<p>
for logstash manually setup<br>
</p>
<pre class="example">
./filebeat setup --template -E output.logstash.enabled=false -E 'output.elasticsearch.hosts=["localhost:9200"]'
</pre></li>

<li><p>
setup kibana dashboards<br>
</p>

<pre class="example">
./filebeat setup --dashboards
</pre></li>

<li><p>
start filebeat<br>
</p>

<p>
sudo chown root filebeat.yml<br>
</p>

<p>
sudo -s<br>
</p>

<p>
nohup /home/manue1/opt/filebeat-6.1.1-linux-x86_64/filebeat -e -c /home/manue1/opt/filebeat-6.1.1-linux-x86_64/filebeat.yml -d "publish" &amp;<br>
</p>

<p>
ps aux |grep beat<br>
</p></li>
</ul>
</div>
</div>

<div id="outline-container-orgfcabc6e" class="outline-4">
<h4 id="orgfcabc6e"><span class="section-number-4">4.4.3</span> metricbeat</h4>
<div class="outline-text-4" id="text-4-4-3">
<ol class="org-ol">
<li><p>
download<br>
</p>

<p>
metricbeat-6.1.1-linux-x86_64.tar.gz<br>
</p></li>

<li>conf<br>

<ul class="org-ul">
<li>metricbeat.yml<br>

<ol class="org-ol">
<li><p>
修改es和kibana的地址<br>
</p>

<p>
如果输出到logstash中，需要关闭直接写入es，并配置logstash监听5044端口<br>
es也要手动加载template<br>
</p>
<pre class="example">
output.logstash:
  # The Logstash hosts
  hosts: ["master:5044"]
</pre></li>

<li>配置template模版<br>
<ul class="org-ul">
<li>Configure template loading<br></li>
<li><p>
Load the template manually<br>
required for Logstash output<br>
</p>
<pre class="example">

sudo ./metricbeat setup --template -E output.logstash.enabled=false -E 'output.elasticsearch.hosts=["master:9200"]'
</pre></li>
</ul></li>
</ol></li>
</ul></li>
</ol>


<ul class="org-ul">
<li>modules.d<br>
目录下面可以配置多种模块<br>
修改 logstash.yml.disabled 为 logstash.yml 启动模块<br></li>

<li><p>
kibana dashboard<br>
</p>

<p>
./metricbeat setup &#x2013;dashboards<br>
</p></li>
</ul>
<ol class="org-ol">
<li><p>
start<br>
</p>

<p>
sudo chown root metricbeat.yml<br>
sudo chown root modules.d/system.yml<br>
sudo ./metricbeat -e -c metricbeat.yml -d "publish"<br>
</p>

<p>
ps aux |grep metricbeat<br>
</p></li>
</ol>


<p>
复制到不同节点部署<br>
</p>
</div>
</div>
<div id="outline-container-org425ef76" class="outline-4">
<h4 id="org425ef76"><span class="section-number-4">4.4.4</span> packetbeat</h4>
<div class="outline-text-4" id="text-4-4-4">
<ol class="org-ol">
<li><p>
download<br>
</p>

<p>
packetbeat-6.1.1-linux-x86_64.tar.gz<br>
</p></li>

<li>config<br>

<ul class="org-ul">
<li><p>
packetbeat.yml<br>
</p>

<p>
logstash &amp; kibana 地址修改<br>
</p></li>

<li><p>
setup template<br>
</p>

<p>
./packetbeat setup &#x2013;template -E output.logstash.enabled=false -E 'output.elasticsearch.hosts=["master:9200"]'<br>
</p></li>

<li><p>
set up kibana dashboard<br>
</p>

<p>
./packetbeat setup &#x2013;dashboards<br>
</p></li>
</ul></li>

<li><p>
start beat<br>
</p>

<p>
sudo chown root packetbeat.yml<br>
</p>

<p>
nohup /home/manue1/opt/packetbeat-6.1.1-linux-x86_64/packetbeat -e -c  /home/manue1/opt/packetbeat-6.1.1-linux-x86_64/packetbeat.yml -d "publish" &amp;<br>
</p></li>
</ol>
</div>
</div>
</div>
</div>

<div id="outline-container-org8502d96" class="outline-2">
<h2 id="org8502d96"><span class="section-number-2">5</span> hive</h2>
<div class="outline-text-2" id="text-5">
<p>
hive只需要安装在集群任意一个节点上即可,这里安装在slave01上<br>
</p>
</div>

<div id="outline-container-org51c3ed6" class="outline-3">
<h3 id="org51c3ed6"><span class="section-number-3">5.1</span> install mariadb</h3>
<div class="outline-text-3" id="text-5-1">
<p>
安装hive前，需要mysql作为外置存储引擎，存放hive元数据(metastore)<br>
</p>

<p>
<a href="http://blog.csdn.net/Nemo____/article/details/72897455">参考安装</a> mysql准备环境<br>
</p>

<ul class="org-ul">
<li><p>
remove mariadb<br>
</p>
<pre class="example">
rpm -qa|grep mariadb         //查询出已安装的mariadb
rpm -e --nodeps 文件名        //卸载 ， 文件名为使用rpm -qa|grep mariadb 命令查出的所有文件
sudo  rpm -e --nodeps mariadb-libscc

</pre></li>

<li><p>
install mariadb<br>
</p>

<pre class="example">
yum install mariadb-server mariadb
systemctl start mariadb  #启动MariaDB
systemctl stop mariadb   #停止MariaDB
systemctl restart mariadb  #重启MariaDB
systemctl enable mariadb  #设置开机启动
mysql -uroot -p #NO PASSWORD
set password for 'root'@'localhost' =password('manue1');  # set new password
grant all privileges on *.* to root@'%'identified by 'manue1';  #远程连接设置

</pre>

<p>
vi /etc/my.cnf<br>
</p>
<pre class="example">
# set utf8
[mysql]
default-character-set=utf8  # NO SPACE
</pre></li>
</ul>
</div>
</div>

<div id="outline-container-org37af443" class="outline-3">
<h3 id="org37af443"><span class="section-number-3">5.2</span> config hive</h3>
<div class="outline-text-3" id="text-5-2">
<p>
配置安装<a href="http://mirrors.shuosc.org/apache/hive/stable-2/">hive</a> 参考： <a href="https://cwiki.apache.org/confluence/display/Hive/GettingStarted">1</a>  <a href="http://blog.csdn.net/jssg_tzw/article/details/72354470">2</a><br>
</p>

<ul class="org-ul">
<li><p>
<b>.bashrc</b><br>
</p>

<p>
hive 环境变量配置<br>
</p>

<pre class="example">
export HIVE_HOME=/home/manue1/opt/apache-hive-2.3.2-bin
export PATH=$PATH:$HIVE_HOME/bin

</pre></li>

<li><p>
<b>metastore conf</b><br>
</p>

<p>
hive元数据存放mysql,为hive建立相应的mysql账户,并赋予足够的权限<br>
</p>

<pre class="example">
mysql -h slave01 -uroot -p
insert into mysql.user (Host,User,Password)values('localhost','hive',password('manue1'));
create database hive;
grant all privileges on hive.* to hive@'%'identified by 'manue1'; 
flush privileges; 

</pre></li>

<li><p>
<b>配置hive-env.sh 文件</b><br>
</p>

<p>
mv hive-env.sh.template hive-env.sh<br>
</p>

<pre class="example">
export HADOOP_HOME=/home/manue1/opt/hadoop-2.7.5
export HIVE_CONF_DIR=/home/manue1/opt/apache-hive-2.3.2-bin/conf
export HIVE_AUX_JARS_PATH=/home/manue1/opt/apache-hive-2.3.2-bin/lib

</pre></li>

<li><p>
<b>hive-site.xml</b><br>
</p>

<pre class="example">
mv hive-default.xml hive-site.xml

</pre>

<ol class="org-ol">
<li><p>
hdfs新建hive数据目录<br>
</p>

<p>
因为在hive-site.xml配置了hive表的数据存放在hdfs上的/user/hive/warehouse内,<br>
</p>

<pre class="example">
&lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;
&lt;value&gt;/user/hive/warehouse&lt;/value&gt;
&lt;name&gt;hive.exec.scratchdir&lt;/name&gt;
&lt;value&gt;/tmp/hive&lt;/value&gt;

</pre>

<p>
所以要在Hadoop集群新建目录，执行命令<br>
</p>

<pre class="example">
[manue1@master conf]$ hadoop fs -mkdir -p /user/hive/warehouse
[manue1@master conf]$ hadoop fs -chmod -R 777 /user/hive/warehouse
[manue1@master conf]$ hadoop fs -mkdir -p /tmp/hive
[manue1@master conf]$ hadoop fs -chmod -R 777 /tmp/hive

</pre></li>

<li><p>
hive-site.xml内mysql相关配置<br>
</p>

<p>
需要java连接mysql的依赖包下载<a href="https://dev.mysql.com/downloads/connector/j/5.1.html">mysql-connector-java-5.1.45-bin.jar</a> <br>
</p>

<pre class="example">
mv mysql-connector-java-5.1.45-bin.jar lib/

</pre>

<pre class="example">
1. javax.jdo.option.ConnectionDriverName，将该name对应的value修改为MySQL驱动类路径：
&lt;property
  &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;
  &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;
&lt;/property&gt;  

2. javax.jdo.option.ConnectionURL，将该name对应的value修改为MySQL的地址：
 &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;
 &lt;value&gt;jdbc:mysql://192.168.56.11:3306/hive?createDatabaseIfNotExist=true&lt;/value&gt;

3.javax.jdo.option.ConnectionUserName，将对应的value修改为MySQL数据库登录名：
&lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;
&lt;value&gt;hive&lt;/value&gt;

4.javax.jdo.option.ConnectionPassword，将对应的value修改为MySQL数据库的登录密码：
&lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;
&lt;value&gt;*******&lt;/value&gt;

</pre></li>

<li><p>
替换${system 等值<br>
</p>

<p>
${system:user.name}都替换为manue1<br>
</p>

<p>
${system:java.io.tmpdir}替换为hive的临时目录 /home/manue1/opt/apache-hive-2.3.2-bin/iotmp,先创建，再替换<br>
</p>

<pre class="example">
[manue1@master apache-hive-2.3.2-bin]$ mkdir iotmp
[manue1@master apache-hive-2.3.2-bin]$ sudo chmod -R 777 iotmp

</pre>

<pre class="example">
:%s/${system:java.io.tmpdir}/\/home\/manue1\/opt\/apache-hive-2.3.2-bin\/iotmp/gg
:%s/${system:user.name}/manue1/gg

</pre></li>
</ol></li>
</ul>

<p>
最后初始化metadata表数据<br>
</p>

<pre class="example">
schematool -initSchema -dbType mysql

</pre>
</div>
</div>

<div id="outline-container-orgc20d3d0" class="outline-3">
<h3 id="orgc20d3d0"><span class="section-number-3">5.3</span> start hive</h3>
<div class="outline-text-3" id="text-5-3">
<p>
Hive的三种启动方式<br>
</p>

<ol class="org-ol">
<li><p>
hive  命令行模式<br>
</p>

<p>
进入hive安装目录，输入bin/hive的执行程序，或者输入 hive &#x2013;service cli<br>
</p>

<p>
用于linux平台命令行查询，查询语句基本跟mysql查询语句类似<br>
</p></li>

<li><p>
hive  web界面的启动方式<br>
</p>

<p>
bin/hive &#x2013;service hwi<br>
</p>

<p>
用于通过浏览器来访问hive，感觉没多大用途，浏览器访问地址是：127.0.0.1:9999/hwi<br>
</p></li>

<li><p>
hive  远程服务 (端口号10000) 启动方式<br>
</p>

<p>
bin/hive &#x2013;service hiveserver2 &amp;<br>
</p>

<p>
用java，python等程序实现通过jdbc等驱动的访问hive就用这种起动方式了，这个是程序员最需要的方式了<br>
</p></li>
</ol>


<p>
此时可以使用beeline 测试jdbc连接<br>
</p>

<pre class="example">
beeline -u jdbc:hive2://slave01:10000 -n manue1 -p mmanue1

</pre>


<p>
问题一:<br>
</p>
<pre class="example">

Connecting to jdbc:hive2://master:10000/default
18/01/10 20:37:17 [main]: WARN jdbc.HiveConnection: Failed to connect to master:10000
Error: Could not open client transport with JDBC Uri: jdbc:hive2://master:10000/default: Failed to open new session: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.authorize.AuthorizationException): User: manue1 is not allowed to impersonate anonymous (state=08S01,code=0)
Beeline version 2.3.2 by Apache Hive
beeline&gt; 

分析 ： 访问权限问题

解决 ：在hdfs 的配置文件core-site.xml中加入如下配置，root为位置填入  User:*  ，etc   hadoop.proxyuser.eamon.hosts

 &lt;property&gt;
   &lt;name&gt;hadoop.proxyuser.manue1.hosts&lt;/name&gt;
   &lt;value&gt;*&lt;/value&gt;
 &lt;/property&gt;
 &lt;property&gt;
  &lt;name&gt;hadoop.proxyuser.manue1.groups&lt;/name&gt;
  &lt;value&gt;*&lt;/value&gt;
&lt;/property&gt;
</pre>

<p>
问题二:<br>
</p>
<pre class="example">
ERROR 1045 (28000): Access denied for user 'hive'@'slave01' (using password: YES)

查看mysql.user表已经存在hive@%，但依然不能访问slave01,最终无解只能添加下面一条

grant all privileges on hive.* to hive@'%'identified by 'manue1';
flush privileges; 

</pre>
</div>
</div>
</div>
<div id="outline-container-org87cae1b" class="outline-2">
<h2 id="org87cae1b"><span class="section-number-2">6</span> sqoop</h2>
<div class="outline-text-2" id="text-6">
<ol class="org-ol">
<li><p>
环境配置<br>
</p>

<p>
从<a href="https://sqoop.apache.org">官网下载</a> 解压安装，配置SQOOP_HOME目录<br>
</p>
<pre class="example">
export SQOOP_HOME=/home/manue1/opt/sqoop-1.4.7.bin__hadoop-2.6.0
export PATH=$SQOOP_HOME/bin:$PATH
</pre>
<p>
拷贝\({SQOOP_HOME}/conf/sqoop-env-template.sh  
    到\){SQOOP_HOME}/conf/sqoop-env.sh，<br>
然后修改sqoop-env.sh<br>
</p>
<pre class="example">
export HADOOP_COMMON_HOME=/home/manue1/opt/hadoop-2.7.5
export HADOOP_MAPRED_HOME=/home/manue1/opt/hadoop-2.7.5
export HIVE_HOME=/home/manue1/opt/apache-hive-2.3.2-bin
</pre></li>

<li><p>
测试连接mysql<br>
</p>

<p>
将连接mysql的jar导入sqoop/lib内<br>
</p>

<pre class="example">
sqoop list-databases --connect jdbc:mysql://slave01:3306/hive --username root --password manue1

</pre></li>
</ol>
</div>
</div>
<div id="outline-container-org9db1495" class="outline-2">
<h2 id="org9db1495"><span class="section-number-2">7</span> zookeeper</h2>
<div class="outline-text-2" id="text-7">
</div>
<div id="outline-container-org4d0af91" class="outline-3">
<h3 id="org4d0af91"><span class="section-number-3">7.1</span> Replicated ZooKeeper configural</h3>
<div class="outline-text-3" id="text-7-1">
<pre class="example">
note

For replicated mode, a minimum of three servers are required, and it is strongly recommended that you have an odd number of servers. If you only have two servers, then you are in a situation where if one of them fails, there are not enough machines to form a majority quorum. Two servers is inherently less stable than a single server, because there are two single points of failure.

  至少3节点,每个zookeeper服务都可以成为leader， follower，observer。
</pre>

<ul class="org-ul">
<li><p>
vi conf/zoo.cfg<br>
</p>

<p>
sudo mkdir -p <i>var/lib/zookeeper<br>
sudo chown manue1:manue1  /var/lib/zookeeper</i> #manue1 user start service<br>
创建 vi /var/lib/zookeeper/myid 内容为node server.x ,如 master为1<br>
</p>

<pre class="example">

tickTime=2000
dataDir=/var/lib/zookeeper
clientPort=2181
initLimit=5
syncLimit=2
server.1=master:2888:3888   #2888 集群互相通信 3888 leader选举
server.2=slave01:2888:3888
server.3=slave02:2888:3888

# ，端口 2181 由 ZooKeeper 客户端使用，用于连接到 ZooKeeper 服务器；端口 2888 由对等 ZooKeeper 服务器使用，用于互相通信；而端口 3888 用于领导者选举

</pre>
<p>
scp -r zookeeper-3.4.10 manue1@slave01:/home/manue1/opt<br>
scp -r zookeeper-3.4.10 manue1@slave02:/home/manue1/opt<br>
</p></li>

<li><p>
start service<br>
</p>

<p>
nohup /home/manue1/opt/zookeeper-3.4.10/bin/zkServer.sh restart<br>
</p>

<p>
ps -ef | grep zookeeper<br>
</p>

<p>
netstat -tnlpa | grep 2181<br>
</p>

<p>
echo state | nc localhost 2181<br>
</p></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org29c71df" class="outline-2">
<h2 id="org29c71df"><span class="section-number-2">8</span> hbase</h2>
<div class="outline-text-2" id="text-8">
<p>
start hadoop &amp; zookeeper<br>
</p>
</div>
<div id="outline-container-org72db9d3" class="outline-3">
<h3 id="org72db9d3"><span class="section-number-3">8.1</span> hbase configural</h3>
<div class="outline-text-3" id="text-8-1">
<ul class="org-ul">
<li><p>
.bashrc<br>
</p>

<pre class="example">
# Hbase Environment Variables
export HBASE_HOME=/home/manue1/opt/hbase-1.2.6/
PATH=$JAVA_HOME/bin:$HADOOP_HOME/bin:$HBASE_HOME/bin:$PATH



</pre></li>

<li><p>
hbase-env.sh<br>
</p>
<pre class="example">
export JAVA_HOME=/home/manue1/opt/jdk8
export HBASE_MANAGES_ZK=false

</pre></li>

<li>hbase-site.xml<br></li>

<li><p>
regionservers<br>
</p>
<pre class="example">
master
slave01
slave02
</pre></li>
</ul>
</div>
</div>


<div id="outline-container-orge6e4751" class="outline-3">
<h3 id="orge6e4751"><span class="section-number-3">8.2</span> hbase start</h3>
<div class="outline-text-3" id="text-8-2">
<p>
nohup /home/manue1/opt/hbase-1.2.6/bin/start-hbase.sh &amp;<br>
</p>

<p>
nohup /home/manue1/opt/hbase-1.2.6/bin/stop-hbase.sh &amp;<br>
</p>


<p>
<a href="http://master:16010/master-status">http://master:16010/master-status</a><br>
</p>
</div>
</div>
</div>
<div id="outline-container-org9d606ad" class="outline-2">
<h2 id="org9d606ad"><span class="section-number-2">9</span> kafka</h2>
<div class="outline-text-2" id="text-9">
</div>
<div id="outline-container-orgbd36419" class="outline-3">
<h3 id="orgbd36419"><span class="section-number-3">9.1</span> kafka configural</h3>
<div class="outline-text-3" id="text-9-1">
<ul class="org-ul">
<li><p>
conf/server.properties<br>
</p>

<pre class="example">
#指定zookeeper的连接信息
zookeeper.connect=master:2181,slave01:2181,slave02:2181

#每个broker相当于一个节点，注意各个节点的broker.id的值必须唯一
broker.id=0

#broker监听端口
listeners=PLAINTEXT://master:9092

log.dir=/var/log/kafka
</pre>
<p>
sudo mkdir -p /var/log/kafka<br>
sudo chown manue1:manue1 /var/log/kafka<br>
</p>

<p>
scp -r kafka_2.11-1.0.0/ manue1@slave01:/home/manue1/opt<br>
scp -r kafka_2.11-1.0.0/ manue1@slave02:/home/manue1/opt<br>
各个节点修改broker.id 和 listeners<br>
</p></li>
</ul>
</div>
</div>

<div id="outline-container-org539271a" class="outline-3">
<h3 id="org539271a"><span class="section-number-3">9.2</span> kafka start service</h3>
<div class="outline-text-3" id="text-9-2">
<ul class="org-ul">
<li><p>
vi .bashrc<br>
</p>
<pre class="example">
# KAFKA_HOME Environment Variables
export KAFKA_HOME=/home/manue1/opt/kafka_2.11-1.0.0
export PATH=$PATH:$KAFKA_HOME/bin

</pre></li>
</ul>


<p>
nohup  $KAFKA_HOME/bin/kafka-server-start.sh $KAFKA_HOME/config/server.properties &amp;<br>
</p>

<p>
$KAFKA_HOME/bin/kafka-server-stop.sh<br>
</p>
<pre class="example">

   虚拟机环境内存不够，配置启动脚本 jvm heap 大小 

[manue1@slave01 config]$ $KAFKA_HOME/bin/kafka-server-start.sh $KAFKA_HOME/config/server.propertie
Java HotSpot(TM) 64-Bit Server VM warning: INFO: os::commit_memory(0x00000000c0000000, 1073741824, 0) failed; error='Cannot allocate memory' (errno=12)


vi kafka-server-start.sh

KAFKA_HEAP_OPTS="-Xmx256M -Xms128M
</pre>

<p>
sudo netstat -anp | grep 9092<br>
</p>
</div>
</div>
<div id="outline-container-org8634632" class="outline-3">
<h3 id="org8634632"><span class="section-number-3">9.3</span> use kafka</h3>
<div class="outline-text-3" id="text-9-3">
<p>
<a href="http://blog.csdn.net/u010297957/article/details/72758765">http://blog.csdn.net/u010297957/article/details/72758765</a><br>
</p>

<p>
producer &amp; consumer<br>
</p>
<pre class="example">

[manue1@master ~]$ $KAFKA_HOME/bin/kafka-topics.sh --create --topic TestTopic001 --partitions 2 --replication-factor 1 --zookeeper master:2181,slave01:2181,slave02:2181
Created topic "TestTopic001".

[manue1@master ~]$ $KAFKA_HOME/bin/kafka-topics.sh --describe --topic TestTopic001 --zookeeper master:2181,slave01:2181,slave02:2181
Topic:TestTopic001	PartitionCount:2	ReplicationFactor:1	Configs:
    Topic: TestTopic001	Partition: 0	Leader: 1	Replicas: 1	Isr: 1
    Topic: TestTopic001	Partition: 1	Leader: 2	Replicas: 2	Isr: 2

[manue1@master logs]$ $KAFKA_HOME/bin/kafka-console-producer.sh --broker-list master:9092,slave01:9092,slave02:9092 --topic TestTopic001
&gt;hi zbr
&gt;what is your name
&gt;i love zbr


[manue1@slave01 kafka_2.11-1.0.0]$ $KAFKA_HOME/bin/kafka-console-consumer.sh --from-beginning --topic TestTopic001 --zookeeper master:2181,slave01:2181,slave02:2181
Using the ConsoleConsumer with old consumer is deprecated and will be removed in a future major release. Consider using the new consumer by passing [bootstrap-server] instead of [zookeeper].
hi zbr
what is your name
i love zbr

</pre>
</div>
</div>
</div>
<div id="outline-container-org6494061" class="outline-2">
<h2 id="org6494061"><span class="section-number-2">10</span> spark</h2>
<div class="outline-text-2" id="text-10">
</div>
<div id="outline-container-org6fd3e2b" class="outline-3">
<h3 id="org6fd3e2b"><span class="section-number-3">10.1</span> spark configural</h3>
<div class="outline-text-3" id="text-10-1">
<p>
<a href="https://data-flair.training/blogs/install-apache-spark-multi-node-cluster/">https://data-flair.training/blogs/install-apache-spark-multi-node-cluster/</a><br>
</p>


<ul class="org-ul">
<li><p>
.bashrc<br>
</p>
<pre class="example">
#SPARK_HOME Environment Variables
export SPARK_HOME=/home/manue1/opt/spark-2.2.1-bin-hadoop2.7
export PATH=$PATH:$SPARK_HOME/bin
</pre></li>

<li><p>
spark-env.sh<br>
</p>

<p>
cp spark-env.sh.template spark-env.sh<br>
</p>

<pre class="example">
export JAVA_HOME=/home/manue1/opt/jdk8

export SCALA_HOME=/home/manue1/opt/scala-2.11.7

export HADOOP_HOME=/home/manue1/opt/hadoop-2.7.5

export HADOOP_CONF_DIR=/home/manue1/opt/hadoop-2.7.5/etc/hadoop

export SPARK_MASTER_IP=master

export SPARK_WORKER_MEMORY=64m

export SPARK_WORKER_CORES=1

export SPARK_WORKER_INSTANCES=2

变量说明
JAVA_HOME：Java安装目录
SCALA_HOME：Scala安装目录
HADOOP_HOME：hadoop安装目录
HADOOP_CONF_DIR：hadoop集群的配置文件的目录
SPARK_MASTER_IP：spark集群的Master节点的ip地址
SPARK_WORKER_MEMORY：每个worker节点能够最大分配给exectors的内存大小
SPARK_WORKER_CORES：每个worker节点所占有的CPU核数目
SPARK_WORKER_INSTANCES：每台机器上开启的worker节点的数目

</pre></li>

<li><p>
slaves<br>
</p>
<pre class="example">
master
slave01
slave02
</pre></li>
</ul>


<p>
slave sync<br>
</p>

<p>
scp -r spark-2.2.1-bin-hadoop2.7/ manue1@192.168.1.109:/home/manue1/opt<br>
</p>
</div>
</div>
<div id="outline-container-org8161813" class="outline-3">
<h3 id="org8161813"><span class="section-number-3">10.2</span> spark start</h3>
<div class="outline-text-3" id="text-10-2">
<p>
nohup sh /home/manue1/opt/hadoop-2.7.5/sbin/start-all.sh &amp;  #启动hdfs即可<br>
</p>

<p>
$SPARK_HOME/sbin/start-all.sh<br>
</p>


<p>
WebUI<br>
<a href="http://master:8080/">http://master:8080/</a><br>
</p>

<p>
spark-shell.sh 运行后可以访问后台执行的任务<br>
<a href="http://master:4040/">http://master:4040/</a><br>
</p>

<p>
spark-submit yarn 管理方式<br>
<a href="http://master:8088/">http://master:8088/</a><br>
</p>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p>
    <a href="">manue1</a> 搭建和维护，使用
    <a href="https://www.gnu.org/software/emacs/">Emacs</a>
    <a href="http://orgmode.org/">Org mode</a> 编辑和构建
</p>
<script src="http://www.langdebuqing.com/css/jquery-2.1.3.min.js"></script>
<script src="http://www.langdebuqing.com/css/main.js"></script>
</div>
</body>
</html>