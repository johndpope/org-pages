<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2019-01-24 Thu 23:02 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>note-linux-centos</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="manue1" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<link rel="stylesheet" type="text/css" href="https://www.manue1.site/css/style.css" />
<!-- <link rel="stylesheet" type="text/css" href="http://www.langdebuqing.com/css/style.css" /> -->
<!-- <link rel="shortcut icon" href="http://www.langdebuqing.com/images/favicon.ico" type="image/x-icon" /> -->
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2018 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="preamble" class="status">
<div id="navbar">
    <ul>
        <li id="site-master"><a href="https://www.manue1.site/">Manue1's Journal</a></li>
        <li><a class="navbar-item" href="https://www.manue1.site/write.html">All Posts</a> </li>
        <li><a class="navbar-item" href="https://www.manue1.site/link.html">Links</a> </li>
        <li class="search">
            <form action="http://google.com/search" method="get" accept-charset="utf-8">
                <input type="search" id="search" name="q" autocomplete="off" maxlength="30" placeholder="Search..">
                <input type="hidden" name="q" value="site:www.manue1.site">
            </form>
        </li>
    </ul>
</div>
</div>
<div id="content">
<h1 class="title">note-linux-centos</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org490268d">1. VboxManage</a></li>
<li><a href="#orgb393f55">2. centos7 basic environment</a>
<ul>
<li><a href="#org9478a1b">2.1. lvm</a></li>
<li><a href="#org96e4fed">2.2. ip</a></li>
<li><a href="#orga0f2035">2.3. sshd</a></li>
<li><a href="#org34481cb">2.4. hostname</a></li>
<li><a href="#org6940d1d">2.5. yum source</a></li>
<li><a href="#org9a88a7c">2.6. ntp</a></li>
<li><a href="#org421fd17">2.7. firewallds</a></li>
<li><a href="#orge8fb225">2.8. disable selinux</a></li>
<li><a href="#org4b7d787">2.9. java  &amp; scala</a></li>
</ul>
</li>
<li><a href="#orgd7d61fe">3. hadoop 集群配置</a>
<ul>
<li><a href="#org111cbbf">3.1. hadoop hbase spark 版本选择</a></li>
<li><a href="#org8564b3a">3.2. 环境准备</a></li>
<li><a href="#orga2c20cb">3.3. 配置hadoop cluster</a></li>
<li><a href="#org36ba49a">3.4. 启动hadoop</a></li>
</ul>
</li>
<li><a href="#org5534892">4. elasticstk 集群</a>
<ul>
<li><a href="#org2df9990">4.1. elasticsearch-611</a></li>
<li><a href="#orgadc077b">4.2. kibana</a></li>
<li><a href="#org076e19f">4.3. logstash</a></li>
<li><a href="#orgd29a6c6">4.4. beats</a>
<ul>
<li><a href="#orgbc6f0c3">4.4.1. topbeat</a></li>
<li><a href="#orgf6de2b3">4.4.2. filebeat</a></li>
<li><a href="#org5449b9f">4.4.3. metricbeat</a></li>
<li><a href="#org2a15685">4.4.4. packetbeat</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org1d8d55d">5. hive</a>
<ul>
<li><a href="#orga0a9e33">5.1. install mariadb</a></li>
<li><a href="#org81c53db">5.2. config hive</a></li>
<li><a href="#orgd8a5063">5.3. start hive</a></li>
</ul>
</li>
<li><a href="#orgefc5e17">6. sqoop</a></li>
<li><a href="#org92e5865">7. zookeeper</a>
<ul>
<li><a href="#orgdffecae">7.1. Replicated ZooKeeper configural</a></li>
</ul>
</li>
<li><a href="#org67b0d04">8. hbase</a>
<ul>
<li><a href="#org80aab88">8.1. hbase configural</a></li>
<li><a href="#org6af7569">8.2. hbase start</a></li>
</ul>
</li>
<li><a href="#org3406069">9. kafka</a>
<ul>
<li><a href="#orgb7be641">9.1. kafka configural</a></li>
<li><a href="#orga80f540">9.2. kafka start service</a></li>
<li><a href="#orgfafc201">9.3. use kafka</a></li>
</ul>
</li>
<li><a href="#org0076e18">10. spark</a>
<ul>
<li><a href="#org4fd5fec">10.1. spark configural</a></li>
<li><a href="#orgf5bd40a">10.2. spark start</a></li>
</ul>
</li>
</ul>
</div>
</div>
<p>
vbox自建centos集群环境，方便本地开发测试,
</p>

<div id="outline-container-org490268d" class="outline-2">
<h2 id="org490268d"><span class="section-number-2">1</span> VboxManage</h2>
<div class="outline-text-2" id="text-1">
<ul class="org-ul">
<li><p>
vms 基本信息
</p>

<p>
VBoxManage showvminfo Centos701
</p></li>

<li><p>
无界面启动vms
</p>

<p>
VBoxManage startvm Centos701 &#x2013;type headless
</p>

<p>
VBoxManage controlvm ubuntu01 savestate # 保存当前状态关闭
</p></li>

<li><p>
修改vms内存总量
</p>

<p>
VBoxManage modifyvm Centos701 &#x2013;memory 1024
</p></li>

<li><p>
修改vms磁盘容量
</p>

<p>
VBoxManage modifyhd Centos702.vdi &#x2013;resize 7168
</p>

<pre class="example">

如果创建过快照，快照的磁盘空间也需要修改
VBoxManage modifyhd \{aea51811-5e79-46e9-b5ba-f66740e6f47b\}.vdi  --resize 10248

</pre></li>

<li><p>
创建快照
</p>

<p>
VBoxManage snapshot ubuntu01 take ubuntu<sub>basic</sub><sub>ssh</sub>
</p>

<p>
VBoxManage snapshot ubuntu01 list
</p>

<p>
VBoxManage snapshot ubuntu01 delete ubuntu<sub>basic</sub>
</p>

<p>
VBoxManage snapshot ubuntu01 restore ubuntu<sub>basic</sub><sub>ssh</sub>
</p></li>

<li><p>
clone vm
</p>

<p>
VBoxManage clonevm  Centos701 &#x2013;snapshot  centos7<sub>basic</sub>  &#x2013;name Centos<sub>basic</sub>
</p></li>
</ul>

<pre class="example">

查看当前虚拟机  VBoxManage list vms  

查看当前正在运行的虚拟机  VBoxManage list runningvms  

启动虚拟机  VBoxManage startvm 虚拟机名  

无前端图形界面方式启动虚拟机  VBoxManage startvm 虚拟机名 --type headless  

</pre>
</div>
</div>

<div id="outline-container-orgb393f55" class="outline-2">
<h2 id="orgb393f55"><span class="section-number-2">2</span> centos7 basic environment</h2>
<div class="outline-text-2" id="text-2">
<p>
centos7 镜像<a href="http://isoredirect.centos.org/centos/7/isos/x86_64/CentOS-7-x86_64-Minimal-1708.iso">下载</a>
</p>
</div>

<div id="outline-container-org9478a1b" class="outline-3">
<h3 id="org9478a1b"><span class="section-number-3">2.1</span> lvm</h3>
<div class="outline-text-3" id="text-2-1">
<p>
在文件中添加要挂载的分区和文件目录可以修改文件
</p>

<p>
/etc/fstab
</p>

<p>
<i>dev/sda5</i>    media/win    ntfs    defaults   02
</p>

<p>
然后 mount -a
</p>

<ol class="org-ol">
<li><p>
查看几块硬盘
</p>

<p>
sudo fdisk -l |grep sd
</p></li>

<li><p>
创建分区
</p>

<p>
虚拟机现有20g的硬盘,使用fdisk划分磁盘
</p>

<p>
sudo fdisk /dev/sda
</p>

<p class="verse">
m  帮助信息<br />
n 创建分区<br />
e 扩展分区    +5G  pppp/pppe<br />
p 打印分区<br />
t 分区类型 L  (lvm)<br />
w 写入保存分区<br />
</p></li>

<li>格式化 分区</li>

<li><p>
LVM
pv &#x2013;&gt; vg &#x2013;&gt; lv
参考:
<a href="http://blog.sina.com.cn/s/blog_b77735d20101e5cn.html">http://blog.sina.com.cn/s/blog_b77735d20101e5cn.html</a>
<a href="http://aurthurxlc.github.io/Aurthur-2017/Centos-7-extend-lvm-volume.html">http://aurthurxlc.github.io/Aurthur-2017/Centos-7-extend-lvm-volume.html</a>
</p>

<pre class="example">
fdisk -l | grep sd
fdisk /dev/sda
partprobe
pvdisplay
pvcreate /dev/sda3
vgdisplay
vgextend centos /dev/sda3
lvdisplay
#lvcreate -L 3.31G -n manue1 centos
#mkfs.xfs /dev/centos/manue1
#lvremove -f /dev/centos/manue1
lvextend -l +100%FREE /dev/centos/root
df -Th
xfs_growfs /dev/centos/root
</pre></li>
</ol>
</div>
</div>
<div id="outline-container-org96e4fed" class="outline-3">
<h3 id="org96e4fed"><span class="section-number-3">2.2</span> ip</h3>
<div class="outline-text-3" id="text-2-2">
<ul class="org-ul">
<li><p>
联网方式: 配置三张网卡
</p>

<ol class="org-ol">
<li>NAT 
网卡1 用来连接外网</li>
<li><p>
Host-only
用来配置静态IP 配置集群服务的时候不需要修改IP
vi <i>etc/sysconfig/network-scripts</i>  
</p>
<pre class="example">
#static assignment
ONBOOT=yes
BOOTPROTO=static
IPADDR=192.168.56.10
NETMASK=255.255.255.0
GATEWAY=192.168.56.1
</pre></li>
<li>Bridge
vbox 自动配置IP，也很方便</li>
</ol>

<p>
这边打算使用网卡1 nat模式连接外网，网卡3的桥接模式与局域网内其他主机通信,网卡二的主机模式搭建集群
</p>

<p>
注意： 网卡二和网卡三的 gateway 字段要注释掉
</p>

<p>
sudo service network restart
</p></li>
</ul>
</div>
</div>

<div id="outline-container-orga0f2035" class="outline-3">
<h3 id="orga0f2035"><span class="section-number-3">2.3</span> sshd</h3>
<div class="outline-text-3" id="text-2-3">
<p>
ssh 连接异常慢
</p>

<p>
sudo vi /etc/ssh/sshd<sub>config</sub>
</p>

<pre class="example">
UseDNS no

</pre>
</div>
</div>

<div id="outline-container-org34481cb" class="outline-3">
<h3 id="org34481cb"><span class="section-number-3">2.4</span> hostname</h3>
<div class="outline-text-3" id="text-2-4">
<p>
永久修改主机名字
</p>

<p>
sudo hostnamectl &#x2013;static set-hostname master
</p>

<p>
sudo vi /etc/hosts
</p>

<pre class="example">
[manue1@localhost ~]$ cat /etc/hostname
 master
[manue1@localhost ~]$ cat /etc/hosts
 127.0.0.1 master
 ::1 master
</pre>
</div>
</div>


<div id="outline-container-org6940d1d" class="outline-3">
<h3 id="org6940d1d"><span class="section-number-3">2.5</span> yum source</h3>
<div class="outline-text-3" id="text-2-5">
<p>
sudo yum -y install wget
</p>

<ul class="org-ul">
<li><p>
备份
</p>

<p>
sudo mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup
</p></li>

<li><p>
设置aliyun source
</p>

<p>
sudo wget -O /etc/yum.repos.d/CentOS-Base.repo <a href="http://mirrors.aliyun.com/repo/Centos-7.repo">http://mirrors.aliyun.com/repo/Centos-7.repo</a>
</p></li>

<li><p>
设置EPLEPEL source
</p>

<p>
sudo wget -P <i>etc/yum.repos.d</i> <a href="http://mirrors.aliyun.com/repo/epel-7.repo">http://mirrors.aliyun.com/repo/epel-7.repo</a>
</p>

<p>
添加后可以像fedora上 yum install packname
</p></li>

<li><p>
清理缓存并生成新的缓存
</p>

<p>
sudo yum clean all  
</p>

<p>
sudo yum makecache  
</p></li>
</ul>
</div>
</div>

<div id="outline-container-org9a88a7c" class="outline-3">
<h3 id="org9a88a7c"><span class="section-number-3">2.6</span> ntp</h3>
<div class="outline-text-3" id="text-2-6">
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">master</th>
<th scope="col" class="org-left">server</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">slave01</td>
<td class="org-left">client</td>
</tr>

<tr>
<td class="org-left">slave02</td>
<td class="org-left">client</td>
</tr>
</tbody>
</table>

<ul class="org-ul">
<li><p>
any nodes
</p>

<p>
sudo yum -y install ntp
</p>

<p>
timedatectl set-timezone Asia/Shanghai   # 设置上海时区
</p></li>

<li><p>
server configural
</p>

<p>
systemctl start ntpd
</p>

<p>
systemctl enable ntpd
</p>

<p>
vi /etc/ntp.conf 
</p>

<pre class="example">

restrict 192.168.56.0 mask 255.255.0.0

server 127.127.1.0

fudge 127.127.1.0 stratum 10
</pre>

<p>
systemctl restart ntpd
</p>

<ul class="org-ul">
<li><p>
client configural
</p>

<p>
systemctl start ntpd
</p>

<p>
systemctl enable ntpd
</p>

<p>
vi /etc/ntp.conf
</p>

<pre class="example">
server 192.168.56.10

</pre>

<p>
netstat -anp | grep 123
</p></li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org421fd17" class="outline-3">
<h3 id="org421fd17"><span class="section-number-3">2.7</span> firewallds</h3>
<div class="outline-text-3" id="text-2-7">
<dl class="org-dl">
<dt>查看状态</dt><dd>systemctl status firewalld</dd>
<dt>关闭</dt><dd>systemctl stop firewalld</dd>
<dt>禁用</dt><dd>systemctl disable firewalld</dd>
</dl>
</div>
</div>
<div id="outline-container-orge8fb225" class="outline-3">
<h3 id="orge8fb225"><span class="section-number-3">2.8</span> disable selinux</h3>
<div class="outline-text-3" id="text-2-8">
<p>
一款为了提高系统安全性的软件：对系统服务，文件权限，网络端口访问有极其严格的限制，
例如：如果对一个文件没有正确安全上下文配置， 甚至你是root用户，你也不能启动某服务
</p>

<p>
sudo vi /etc/sysconfig/selinux
</p>
<pre class="example">
selinux = disable

</pre>
</div>
</div>
<div id="outline-container-org4b7d787" class="outline-3">
<h3 id="org4b7d787"><span class="section-number-3">2.9</span> java  &amp; scala</h3>
<div class="outline-text-3" id="text-2-9">
<p>
基础环境用root 配置在/etc/profile 自启动环境文件内
</p>

<p>
refer : <a href="https://www.mtyun.com/library/how-to-setup-scala-on-centos7">1</a>
</p>

<ul class="org-ul">
<li>java rpm install

<ol class="org-ol">
<li><a href="http://www.oracle.com/technetwork/java/javase/downloads/index.html">download</a></li>
<li><p>
install
</p>
<p class="verse">
sudo rpm -ivh jdk-8u144-linux-x64.rpm<br />
sudo rpm -aq | grep jdk<br />
sudo rpm -e jdk   无效<br />
sudo yum remove jdk<br />
</p>

<p>
sudo vi /etc/profile
</p>

<pre class="example">
#JAVA_HOME
export JAVA_HOME=/usr/java/jdk1.8.0_144
export CLASSPATH=.:$JAVA_HOME/jre/lib/rt.jar:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
export PATH=$PATH:$JAVA_HOME/bin
</pre></li>
</ol></li>
<li><p>
java 离线包安装
</p>

<p>
tar -zxvf jdk-8u151-linux-x64.tar.gz
</p>

<p>
vi /etc/profile
</p>
<pre class="example">
#JAVA_HOME
JAVA_HOME=/home/manue1/opt/jdk8
PATH=$PATH:$JAVA_HOME/bin
export JAVA_HOME PATH
</pre></li>
<li><p>
scala 离线包安装
</p>

<p>
当前最新版本
</p>

<p>
tar zxvf scala-2.11.7.tgz
</p>

<pre class="example">
SCALA_HOME=/home/manue1/opt/scala-2.11.7
PATH=$PATH:$SCALA_HOME/bin
export SCALA_HOME PATH
</pre></li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orgd7d61fe" class="outline-2">
<h2 id="orgd7d61fe"><span class="section-number-2">3</span> hadoop 集群配置</h2>
<div class="outline-text-2" id="text-3">
</div>
<div id="outline-container-org111cbbf" class="outline-3">
<h3 id="org111cbbf"><span class="section-number-3">3.1</span> hadoop hbase spark 版本选择</h3>
<div class="outline-text-3" id="text-3-1">
<ul class="org-ul">
<li><p>
hbase 支持 hadoop 版本对照表
</p>

<p>
The 1.2.x series is the current stable release line
</p>

<p>
<a href="http://www-us.apache.org/dist/hbase/">http://www-us.apache.org/dist/hbase/</a>
</p>

<p>
下面查看1.2.x 需要的hadoop版本
</p>

<p>
<a href="http://hbase.apache.org/book.html#arch.overview">http://hbase.apache.org/book.html#arch.overview</a>
</p>

<p>
crtl + F  "s" 搜索页面
</p>

<p>
选择 Hadoop-2.7.1+
</p></li>

<li><p>
spark 支持 hadoop
</p>

<p>
<a href="http://spark.apache.org/downloads.html">http://spark.apache.org/downloads.html</a>
</p>

<p>
官方下载页面可以手动选择
</p></li>

<li><p>
hive 支持 hadoop
</p>

<p>
<a href="https://hive.apache.org/downloads.html">https://hive.apache.org/downloads.html</a>
</p>

<p>
稳定版下载地址
</p>

<p>
<a href="http://mirrors.shuosc.org/apache/hive/stable-2/">http://mirrors.shuosc.org/apache/hive/stable-2/</a>
</p></li>

<li><p>
zookeeper
</p>

<p>
下载稳定版即可
</p>

<p>
<a href="http://mirrors.shuosc.org/apache/zookeeper/stable/">http://mirrors.shuosc.org/apache/zookeeper/stable/</a>
</p></li>
</ul>
</div>
</div>

<div id="outline-container-org8564b3a" class="outline-3">
<h3 id="org8564b3a"><span class="section-number-3">3.2</span> 环境准备</h3>
<div class="outline-text-3" id="text-3-2">
<p>
三台vbox 虚拟centos7 配置 java scala 环境 关闭防火墙和selinux
</p>

<ul class="org-ul">
<li><p>
cluster
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">hostname</th>
<th scope="col" class="org-right">ip</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">master</td>
<td class="org-right">192.168.56.10</td>
</tr>
</tbody>
<tbody>
<tr>
<td class="org-left">slave01</td>
<td class="org-right">192.168.56.11</td>
</tr>
</tbody>
<tbody>
<tr>
<td class="org-left">slave02</td>
<td class="org-right">192.168.56.12</td>
</tr>
</tbody>
</table></li>

<li><p>
disable ipv6
</p>

<p>
sudo vi /etc/sysctl.conf
</p>

<p>
添加下面内容
</p>

<pre class="example">

# disable ipv6
net.ipv6.conf.all.disable_ipv6 = 1
net.ipv6.conf.default.disable_ipv6 = 1
net.ipv6.conf.lo.disable_ipv6 = 1

</pre>

<p>
解决master:50070 页面找不到live node 
</p>

<p>
解决 connection exception
</p>

<pre class="example">
17//23 23:19:48 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
ls:all From slave01/127.0.0.1 to master:9000 failed on connection exception: java.net.ConnectException: 拒绝连接; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused

</pre></li>

<li><p>
hostname &amp; host
</p>

<p>
三台主机都要,修改主机名,修改/etc/hosts 互相添加hostname访问别名
</p>

<p>
注意； #127.0.0.1 master 这样的映射一定要注释掉,master:8088无法访问最终定位到这里了
</p>
<pre class="example">
#ceos7 cluster
19268.56.10 master
19268.56.11 slave01
19268.56.12 slave02
</pre></li>

<li><p>
免登录验证
</p>
<p class="verse">
ssh-keygen -t rsa<br />
ssh-copy-id -i ~/.ssh/id<sub>rsa.pub</sub> manue1@slave01<br />
ssh-copy-id -i ~/.ssh/id<sub>rsa.pub</sub> manue1@slave02<br />
ssh-copy-id -i ~/.ssh/id<sub>rsa.pub</sub> manue1@master<br />
</p></li>

<li>download hadoop
tar -zxvf hadoop-2.7.5.tar.gz</li>
</ul>
</div>
</div>
<div id="outline-container-orga2c20cb" class="outline-3">
<h3 id="orga2c20cb"><span class="section-number-3">3.3</span> 配置hadoop cluster</h3>
<div class="outline-text-3" id="text-3-3">
<ul class="org-ul">
<li><p>
hadoop<sub>home</sub>
三台节点都需要配置
</p>

<p>
vi .bashrc
</p>
<pre class="example">
# Hadoop Environment Variables
export HADOOP_HOME=/home/manue1/opt/hadoop-2.7.5
export HADOOP_INSTALL=$HADOOP_HOME
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin
export HADOOP_OPTS="$HADOOP_OPTS -Djava.library.path=$HADOOP_HOME/lib/native" #解决WARN util.NativeCodeLoader: Unable to load native-hadoop library
export CATALINA_BASE=$HADOOP_HOME/share/hadoop/httpfs/tomcat #支持httpfs rest api

</pre></li>

<li><p>
master
</p>

<p>
<i>home/manue1/opt/hadoop-2.7.5/etc/hadoop</i> 下6个配置文件
</p>

<ol class="org-ol">
<li><p>
core-site.xml
</p>

<pre class="example">
&lt;configuration&gt;
    &lt;!-- 指定HDFS老大（namenode）的通信地址 --&gt;
    &lt;property&gt;
        &lt;name&gt;fs.defaultFS&lt;/name&gt;
        &lt;value&gt;hdfs://master:9000&lt;/value&gt;
    &lt;/property&gt;
    &lt;!-- 指定hadoop运行时产生文件的存储路径 --&gt;
    &lt;property&gt;
        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
        &lt;value&gt;file:/home/manue1/opt/hadoop-2.7.5/tmp&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;
    &lt;!--开启httpfs实现一种匿名的方式登陆hdfs文件系统 端口14000
        manue1用户为hdfs的超级用户 hive启动用户
    --&gt;

    &lt;property&gt;
         &lt;name&gt;hadoop.proxyuser.manue1.hosts&lt;/name&gt;
         &lt;value&gt;*&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;hadoop.proxyuser.manue1.groups&lt;/name&gt;
        &lt;value&gt;*&lt;/value&gt;
    &lt;/property&gt;


</pre></li>

<li><p>
hdfs-site.xml
</p>

<pre class="example">
&lt;configuration&gt;
        &lt;!-- 设置namenode的http通讯地址 --&gt;
        &lt;property&gt;
                &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;
                &lt;value&gt;master:50090&lt;/value&gt;
        &lt;/property&gt;
        &lt;!-- 设置hdfs副本数量 --&gt;
        &lt;property&gt;
                &lt;name&gt;dfs.replication&lt;/name&gt;
                &lt;value&gt;1&lt;/value&gt;
        &lt;/property&gt;
         &lt;!-- 设置namenode存放的路径 --&gt;
        &lt;property&gt;
                &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;
                &lt;value&gt;file:/home/manue1/opt/hadoop-2.7.5/tmp/dfs/name&lt;/value&gt;
        &lt;/property&gt;
         &lt;!-- 设置datanode存放的路径 --&gt;
        &lt;property&gt;
                &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
                &lt;value&gt;file:/home/manue1/opt/hadoop-2.7.5/tmp/dfs/data&lt;/value&gt;
        &lt;/property&gt;
&lt;/configuration&gt;


</pre></li>

<li><p>
mapred-site.xml
</p>

<p>
mv mapred-site.xml.template mapred-site.xml
</p>

<pre class="example">
&lt;configuration&gt;
        &lt;!-- 通知框架MR使用YARN --&gt;
        &lt;property&gt;
                &lt;name&gt;mapreduce.framework.name&lt;/name&gt;
                &lt;value&gt;yarn&lt;/value&gt;
        &lt;/property&gt;
        &lt;property&gt;
                &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;
                &lt;value&gt;master:10020&lt;/value&gt;
        &lt;/property&gt;
        &lt;property&gt;
                &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;
                &lt;value&gt;master:19888&lt;/value&gt;
        &lt;/property&gt;
&lt;/configuration&gt;

</pre></li>

<li><p>
yarn-site.xml
</p>

<pre class="example">
&lt;configuration&gt;
 &lt;!-- 设置 resourcemanager 在哪个节点--&gt;
&lt;!-- Site specific YARN configuration properties --&gt;
        &lt;property&gt;
                &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;
                &lt;value&gt;master&lt;/value&gt;
        &lt;/property&gt;
         &lt;!-- reducer取数据的方式是mapreduce_shuffle --&gt;
        &lt;property&gt;
                &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
                &lt;value&gt;mapreduce_shuffle&lt;/value&gt;
        &lt;/property&gt;
        &lt;!--所有主机访问yarn管理界面--&gt;
        &lt;property&gt; 
                &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt;
                &lt;value&gt;0.0.0.0:8088&lt;/value&gt;
        &lt;/property&gt;

&lt;/configuration&gt;

</pre></li>

<li><p>
slaves
</p>

<pre class="example">
slave01
slave02
</pre></li>

<li><p>
hadoop-env.sh
</p>

<p>
修改
export JAVA<sub>HOME</sub>=/home/manue1/opt/jdk8
</p></li>
</ol></li>

<li><p>
slaves
</p>

<p>
复制master节点配置好的安装包到指定slaves目录
</p>

<pre class="example">
tar -zcvf hadoop-2.7.5_conf_finshed.tar.gz hadoop-2.7.5/
scp hadoop-2.7.5_conf_finshed.tar.gz manue1@slave02:/home/manue1/opt/

</pre></li>
</ul>
</div>
</div>

<div id="outline-container-org36ba49a" class="outline-3">
<h3 id="org36ba49a"><span class="section-number-3">3.4</span> 启动hadoop</h3>
<div class="outline-text-3" id="text-3-4">
<p>
第一次启动要执行格式化，之后启动不用执行这条
</p>
<pre class="example">
hdfs namenode -format 

</pre>

<p>
启动命令:
</p>
<pre class="example">
start-dfs.sh
start-yarn.sh
mr-jobhistory-daemon.sh start historyserver  ??
httpfs.sh start

</pre>


<ul class="org-ul">
<li><p>
master
</p>
<pre class="example">
manue1@master sbin]$ jps
2034 NameNode
2483 Jps
15754 Bootstrap  #httpfs
1652 ResourceManager
2188 SecondaryNameNode
2447 JobHistoryServer

</pre></li>

<li><p>
slaves
</p>
<pre class="example">
[manue1@slave01 hadoop]$ jps
1360 DataNode
1430 NodeManager
1516 Jps
</pre></li>
</ul>

<p>
hadoop cluster状态展示界面 webhdfs
</p>

<pre class="example">
http://master:50070/
curl "http://master:50070/webhdfs/v1/?op=liststatus&amp;user.name=manue1"

</pre>

<p>
httpfs rest api 配置HA的时候找不到namenode可以采用httpfs
</p>

<pre class="example">
http://master:14000/
curl "http://master:14000/webhdfs/v1/?op=liststatus&amp;user.name=manue1"

</pre>

<p>
yarn 管理界面
</p>

<pre class="example">
http://master:8088

</pre>
</div>
</div>
</div>

<div id="outline-container-org5534892" class="outline-2">
<h2 id="org5534892"><span class="section-number-2">4</span> elasticstk 集群</h2>
<div class="outline-text-2" id="text-4">
</div>
<div id="outline-container-org2df9990" class="outline-3">
<h3 id="org2df9990"><span class="section-number-3">4.1</span> elasticsearch-611</h3>
<div class="outline-text-3" id="text-4-1">
<blockquote>
<ol class="org-ol">
<li><p>
环境准备
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-right" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">hostname</th>
<th scope="col" class="org-right">ip</th>
<th scope="col" class="org-left">&#xa0;</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">master</td>
<td class="org-right">192.168.56.10</td>
<td class="org-left">masternode</td>
</tr>
</tbody>
<tbody>
<tr>
<td class="org-left">slave01</td>
<td class="org-right">192.168.56.11</td>
<td class="org-left">datanode</td>
</tr>
</tbody>
<tbody>
<tr>
<td class="org-left">slave02</td>
<td class="org-right">192.168.56.12</td>
<td class="org-left">datanode</td>
</tr>
</tbody>
</table>

<ul class="org-ul">
<li><p>
生产环境配置: 
</p>

<p>
10节点 6个master候选节点
</p>

<p>
1节点 4G + lucene 3G = 8G
</p>

<p>
10T硬盘，最多存5T数据
</p>

<p>
4c + 8g + 1T * 10台 = 10T 
</p>

<p>
4c + 16g + 2T * 5台 = 10T
</p></li>

<li>java 环境配置，关闭firewalld,主机名配置,官网下载elasticsearch-6.1.1.tar.gz</li>

<li>普通用户下安装es  username:manue1</li>

<li><p>
系统设置
</p>

<p>
sudo -s 切换到root下执行
</p>

<pre class="example">

sed -e '$a vm.max_map_count = 262144' -i /etc/sysctl.conf

sysctl -p



echo "ulimit -SHn 1048576" &gt;&gt; /etc/rc.local

sed -e '$a DefaultLimitCORE=infinity\nDefaultLimitNOFILE=1048576\nDefaultLimitNPROC=1048576' -i /etc/systemd/system.conf

cat &gt;&gt; /etc/security/limits.conf &lt;&lt; EOF

 *           soft   nofile       1048576

 *           hard   nofile       1048576

 *           soft   nproc        1048576

 *           hard   nproc        1048576

EOF

sed -i 's/4096/1048576/' /etc/security/limits.d/20-nproc.conf

sed -e '/root       soft    nproc     unlimited/a\*           soft   nofile       1048576\n*           hard   nofile       1048576' -i /etc/security/limits.d/20-nproc.conf

</pre>

<p>
修改系统配置文件后，重启系统生效
</p></li>
</ul></li>

<li><p>
配置elasticsearch
</p>

<ul class="org-ul">
<li><p>
elasticsearch.yml          # els的配置文件
</p>

<pre class="example">
cluster.name: manue1-es-cluster  #集群名称

node.name: master-node           #节点名称

node.data: false
node.master: true  #建议直接不设置，默认两个都为true.

path.data: /Home/Manue1/Opt/Elasticsearch-6.1.1/Els/Data  #数据存储目录

path.logs: /home/manue1/opt/elasticsearch-6.1.1/els/log   #日志存储目录

network.bind_host: 0.0.0.0   #master节点配置 ”0.0.0.0”，允许所有网络接口访问
network.publish_host: master # 集群通信

gateway.recover_after_nodes: 3  #值为n，网关控制在n个节点启动之后才恢复整个集群, 3节&gt;点启动后1分钟
gateway.recover_after_time: 1m

indices.recovery.max_bytes_per_sec: 20mb  #恢复数据时,限制的宽带流量,如果是0就是无限制

node.max_local_storage_nodes: 1                  #值为n，一个系统中最多启用节点个数为n

http.port: 9200                 # 对外提供服务的端口，9300为集群服务的端口


</pre></li>

<li><p>
jvm.options                # JVM相关的配置，内存大小等等
</p>

<pre class="example">
-Xms128M
-Xmx128M
-Xmx1g与-Xms1gJVM的最大最小内存。如果太小会导致Elasticsearch刚刚启动就立刻停止。太大会拖慢系统本身
</pre></li>

<li>log4j2.properties          # 日志系统定义</li>
</ul>

<p>
将配置好的elasticsearch 打包传到各个节点，需要注意的是，如果配置过程中运行产生的data/nodes/0 文件
一定要删掉，再打包使用，否则各个节点启动成功了，无法加入到集群，节点id冲突
报错信息: <code>with the same id but is a different node instance</code>
</p></li>

<li><p>
启动elasticsearch
</p>

<p>
su manue1
</p>

<p>
vi  /home/manue1/opt/elasticsearch-6.1.1/bin/elasticsearch
</p>

<pre class="example">
ES_HEAP_SIZE=128m
MAX_OPEN_FILES=262144
</pre>

<p>
nohup ./bin/elasticsearch -d 
</p>

<p>
关闭 ps -ef |grep elasticsearch|awk '{print $2}'|xargs kill -9
</p></li>

<li><p>
refer
</p>

<p>
<a href="https://blog.csdn.net/thomas0yang/article/details/55518105#%E5%86%85%E5%AD%98">1</a> <a href="https://www.elastic.co/guide/cn/elasticsearch/guide/cn/important-configuration-changes.html#_%E6%9C%80%E5%B0%8F%E4%B8%BB%E8%8A%82%E7%82%B9%E6%95%B0">2</a> <a href="https://zhuanlan.zhihu.com/p/35291900">3</a>
</p></li>
</ol>

<p>
`
</p>
</blockquote>
</div>
</div>

<div id="outline-container-orgadc077b" class="outline-3">
<h3 id="orgadc077b"><span class="section-number-3">4.2</span> kibana</h3>
<div class="outline-text-3" id="text-4-2">
<p>
配置在es的非数据节点上: 192.168.56.10
</p>

<p>
修改 config/kibana.yml
</p>
<pre class="example">
server.host: "0.0.0.0" #不同网卡网段能够访问
elasticsearch.url: "http://master:9200"
</pre>

<p>
启动： nohup  bin/kibana  &amp;
</p>

<p>
关闭: ps -ef |grep kibana |awk '{print $2}'|xargs kill -9
</p>

<p>
ss -lnp | grep 5601
</p>
</div>
</div>

<div id="outline-container-org076e19f" class="outline-3">
<h3 id="org076e19f"><span class="section-number-3">4.3</span> logstash</h3>
<div class="outline-text-3" id="text-4-3">
<ol class="org-ol">
<li>download
logstash-6.1.1.tar.gz</li>
<li>config

<ul class="org-ul">
<li><p>
创建logstash-conf 目录
beat的配置文件
vi beats.conf
</p>
<pre class="example">
input {
  beats {
    port =&gt; 5044
  }
}

# The filter part of this file is commented out to indicate that it is
# optional.
# filter {
#
# }

output {
  elasticsearch {
    hosts =&gt; "master:9200"
    manage_template =&gt; false
    index =&gt; "%{[@metadata][beat]}-%{[@metadata][version]}-%{+YYYY.MM.dd}" 
  }
}

</pre></li>

<li>jvm.options
修改 xms xmx 最大最小jvm 为256M 比es测试集群吃内存多</li>

<li>logstash.yml</li>
</ul></li>

<li>start logstash

<ul class="org-ul">
<li>bin/logstash -e 'input { stdin { } } output { stdout {} }'
测试启动</li>

<li><p>
./bin/logstash -f logstash-conf/beats.conf &amp;
</p>

<p>
配置文件启动
</p>

<p>
sudo netstat -anp | grep 5044
</p></li>
</ul></li>
</ol>
</div>
</div>
<div id="outline-container-orgd29a6c6" class="outline-3">
<h3 id="orgd29a6c6"><span class="section-number-3">4.4</span> beats</h3>
<div class="outline-text-3" id="text-4-4">
</div>
<div id="outline-container-orgbc6f0c3" class="outline-4">
<h4 id="orgbc6f0c3"><span class="section-number-4">4.4.1</span> topbeat</h4>
<div class="outline-text-4" id="text-4-4-1">
<p>
5.x版本后弃用了
</p>
<ol class="org-ol">
<li>下载
topbeat-1.3.1-x86<sub>64.tar.gz</sub></li>

<li>配置

<ul class="org-ul">
<li>topbeat</li>

<li>topbeat.template.json  
topbeat自带的模版，用来创建存放收集数据的索引结构</li>

<li><p>
topbeat.yml
</p>
<pre class="example">
input:
  period: 10           #默认10秒收集一次
  procs: [".*"]   #定义正则表达式，以匹配你所要监控的进程。默认是所有正在运行的进程都进行监控。
  stats:
    system: true
    proc: true
    filesystem: true
output:
  elasticsearch:
    hosts: ["master:9200"]
shipper:
logging:
  files:
</pre></li>
</ul></li>
</ol>


<ol class="org-ol">
<li>es导入模版
导入topbeat自带的模版，用来创建存放收集数据的索引结构

<ul class="org-ul">
<li><p>
Configuring Template Loading - supported for Elasticsearch output only
</p>
<pre class="example">
ERR Failed to perform any bulk index operations: 406 Not Acceptable
错误应该是模版和6.0版本不匹配了，官网没有更新
再去官网查看，topbeat 从5.0 已经被 Metricbeat替换了
</pre></li>

<li>Loading the Template Manually - required for Logstash output</li>
</ul></li>
</ol>


<ol class="org-ol">
<li>kibana</li>

<li><p>
启动topbeat节点
</p>

<p>
sudo ./topbeat -e -c topbeat.yml -d "publish"
</p></li>
</ol>
</div>
</div>
<div id="outline-container-orgf6de2b3" class="outline-4">
<h4 id="orgf6de2b3"><span class="section-number-4">4.4.2</span> filebeat</h4>
<div class="outline-text-4" id="text-4-4-2">
<ol class="org-ol">
<li><p>
download
</p>

<p>
filebeat-6.1.1-linux-x86<sub>64.tar.gz</sub>
</p>

<p>
<a href="https://download.elastic.co/demos/logstash/gettingstarted/logstash-tutorial.log.gz">https://download.elastic.co/demos/logstash/gettingstarted/logstash-tutorial.log.gz</a>
logstash-tutorial.log.gz apache 的日志文件样本
</p></li>

<li>config

<ul class="org-ul">
<li><p>
filebeat.yml
</p>
<pre class="example">
- type: log
  # Change to true to enable this prospector configuration.
  enabled: true
  # Paths that should be crawled and fetched. Glob based paths.
  paths:
    - /var/log/*.log
    - /home/manue1/opt/source/*.log
    #- c:\programdata\elasticsearch\logs\*


output.logstash:
  # The Logstash hosts
  hosts: ["master:5044"]


setup.kibana:

  host: "master:5601"         

</pre></li>

<li><p>
modules
</p>

<p>
sudo chown -R root /home/manue1/opt/filebeat-6.1.1-linux-x86<sub>64</sub>/module
</p>

<p>
sudo chown -R root /home/manue1/opt/filebeat-6.1.1-linux-x86<sub>64</sub>/modules.d
</p>

<ul class="org-ul">
<li><p>
Enable modules when you run Filebeatedit
</p>

<p>
sudo ./filebeat -e &#x2013;modules system,nginx,mysql  
</p>

<p>
./filebeat -e &#x2013;modules nginx -M "nginx.access.var.paths=[/var/log/nginx/access.log*]"
</p></li>

<li><p>
filebeat.yml
</p>

<p>
sudo ./filebeat modules list
</p>

<p>
sudo ./filebeat modules enable system 
</p>

<pre class="example">
默认配置读取所有enable
filebeat.modules:
- module: nginx
- module: mysql
- module: system

</pre></li>
</ul></li>
</ul></li>
</ol>





<ul class="org-ul">
<li><p>
setup template
</p>

<p>
for logstash manually setup
</p>
<pre class="example">
./filebeat setup --template -E output.logstash.enabled=false -E 'output.elasticsearch.hosts=["localhost:9200"]'
</pre></li>

<li><p>
setup kibana dashboards
</p>

<pre class="example">
./filebeat setup --dashboards
</pre></li>

<li><p>
start filebeat
</p>

<p>
sudo chown root filebeat.yml 
</p>

<p>
sudo -s
</p>

<p>
nohup /home/manue1/opt/filebeat-6.1.1-linux-x86<sub>64</sub>/filebeat -e -c /home/manue1/opt/filebeat-6.1.1-linux-x86<sub>64</sub>/filebeat.yml -d "publish" &amp;
</p>

<p>
ps aux |grep beat
</p></li>
</ul>
</div>
</div>

<div id="outline-container-org5449b9f" class="outline-4">
<h4 id="org5449b9f"><span class="section-number-4">4.4.3</span> metricbeat</h4>
<div class="outline-text-4" id="text-4-4-3">
<ol class="org-ol">
<li><p>
download
</p>

<p>
metricbeat-6.1.1-linux-x86<sub>64.tar.gz</sub>
</p></li>

<li>conf

<ul class="org-ul">
<li>metricbeat.yml

<ol class="org-ol">
<li><p>
修改es和kibana的地址
</p>

<p>
如果输出到logstash中，需要关闭直接写入es，并配置logstash监听5044端口
es也要手动加载template
</p>
<pre class="example">
output.logstash:
  # The Logstash hosts
  hosts: ["master:5044"]
</pre></li>

<li>配置template模版
<ul class="org-ul">
<li>Configure template loading</li>
<li><p>
Load the template manually  
required for Logstash output
</p>
<pre class="example">

sudo ./metricbeat setup --template -E output.logstash.enabled=false -E 'output.elasticsearch.hosts=["master:9200"]'
</pre></li>
</ul></li>
</ol></li>
</ul></li>
</ol>


<ul class="org-ul">
<li>modules.d 
目录下面可以配置多种模块
修改 logstash.yml.disabled 为 logstash.yml 启动模块</li>

<li><p>
kibana dashboard
</p>

<p>
./metricbeat setup &#x2013;dashboards
</p></li>
</ul>
<ol class="org-ol">
<li><p>
start 
</p>

<p>
sudo chown root metricbeat.yml 
sudo chown root modules.d/system.yml 
sudo ./metricbeat -e -c metricbeat.yml -d "publish"
</p>

<p>
ps aux |grep metricbeat
</p></li>
</ol>


<p>
复制到不同节点部署
</p>
</div>
</div>
<div id="outline-container-org2a15685" class="outline-4">
<h4 id="org2a15685"><span class="section-number-4">4.4.4</span> packetbeat</h4>
<div class="outline-text-4" id="text-4-4-4">
<ol class="org-ol">
<li><p>
download 
</p>

<p>
packetbeat-6.1.1-linux-x86<sub>64.tar.gz</sub>
</p></li>

<li>config 

<ul class="org-ul">
<li><p>
packetbeat.yml
</p>

<p>
logstash &amp; kibana 地址修改
</p></li>

<li><p>
setup template
</p>

<p>
./packetbeat setup &#x2013;template -E output.logstash.enabled=false -E 'output.elasticsearch.hosts=["master:9200"]'
</p></li>

<li><p>
set up kibana dashboard
</p>

<p>
./packetbeat setup &#x2013;dashboards
</p></li>
</ul></li>

<li><p>
start beat
</p>

<p>
sudo chown root packetbeat.yml 
</p>

<p>
nohup /home/manue1/opt/packetbeat-6.1.1-linux-x86<sub>64</sub>/packetbeat -e -c  /home/manue1/opt/packetbeat-6.1.1-linux-x86<sub>64</sub>/packetbeat.yml -d "publish" &amp;
</p></li>
</ol>
</div>
</div>
</div>
</div>

<div id="outline-container-org1d8d55d" class="outline-2">
<h2 id="org1d8d55d"><span class="section-number-2">5</span> hive</h2>
<div class="outline-text-2" id="text-5">
<p>
hive只需要安装在集群任意一个节点上即可,这里安装在slave01上
</p>
</div>

<div id="outline-container-orga0a9e33" class="outline-3">
<h3 id="orga0a9e33"><span class="section-number-3">5.1</span> install mariadb</h3>
<div class="outline-text-3" id="text-5-1">
<p>
安装hive前，需要mysql作为外置存储引擎，存放hive元数据(metastore)
</p>

<p>
<a href="http://blog.csdn.net/Nemo____/article/details/72897455">参考安装</a> mysql准备环境
</p>

<ul class="org-ul">
<li><p>
remove mariadb
</p>
<pre class="example">
rpm -qa|grep mariadb         //查询出已安装的mariadb
rpm -e --nodeps 文件名        //卸载 ， 文件名为使用rpm -qa|grep mariadb 命令查出的所有文件
sudo  rpm -e --nodeps mariadb-libscc

</pre></li>

<li><p>
install mariadb
</p>

<pre class="example">
yum install mariadb-server mariadb
systemctl start mariadb  #启动MariaDB
systemctl stop mariadb   #停止MariaDB
systemctl restart mariadb  #重启MariaDB
systemctl enable mariadb  #设置开机启动
mysql -uroot -p #NO PASSWORD
set password for 'root'@'localhost' =password('manue1');  # set new password
grant all privileges on *.* to root@'%'identified by 'manue1';  #远程连接设置

</pre>

<p>
vi /etc/my.cnf
</p>
<pre class="example">
# set utf8
[mysql]
default-character-set=utf8  # NO SPACE
</pre></li>
</ul>
</div>
</div>

<div id="outline-container-org81c53db" class="outline-3">
<h3 id="org81c53db"><span class="section-number-3">5.2</span> config hive</h3>
<div class="outline-text-3" id="text-5-2">
<p>
配置安装<a href="http://mirrors.shuosc.org/apache/hive/stable-2/">hive</a> 参考： <a href="https://cwiki.apache.org/confluence/display/Hive/GettingStarted">1</a>  <a href="http://blog.csdn.net/jssg_tzw/article/details/72354470">2</a>
</p>

<ul class="org-ul">
<li><p>
<b>.bashrc</b>
</p>

<p>
hive 环境变量配置
</p>

<pre class="example">
export HIVE_HOME=/home/manue1/opt/apache-hive-2.3.2-bin
export PATH=$PATH:$HIVE_HOME/bin

</pre></li>

<li><p>
<b>metastore conf</b>
</p>

<p>
hive元数据存放mysql,为hive建立相应的mysql账户,并赋予足够的权限
</p>

<pre class="example">
mysql -h slave01 -uroot -p
insert into mysql.user (Host,User,Password)values('localhost','hive',password('manue1'));
create database hive;
grant all privileges on hive.* to hive@'%'identified by 'manue1'; 
flush privileges; 

</pre></li>

<li><p>
<b>配置hive-env.sh 文件</b>
</p>

<p>
mv hive-env.sh.template hive-env.sh
</p>

<pre class="example">
export HADOOP_HOME=/home/manue1/opt/hadoop-2.7.5
export HIVE_CONF_DIR=/home/manue1/opt/apache-hive-2.3.2-bin/conf
export HIVE_AUX_JARS_PATH=/home/manue1/opt/apache-hive-2.3.2-bin/lib

</pre></li>

<li><p>
<b>hive-site.xml</b>
</p>

<pre class="example">
mv hive-default.xml hive-site.xml

</pre>

<ol class="org-ol">
<li><p>
hdfs新建hive数据目录
</p>

<p>
因为在hive-site.xml配置了hive表的数据存放在hdfs上的/user/hive/warehouse内,
</p>

<pre class="example">
&lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;
&lt;value&gt;/user/hive/warehouse&lt;/value&gt;
&lt;name&gt;hive.exec.scratchdir&lt;/name&gt;
&lt;value&gt;/tmp/hive&lt;/value&gt;

</pre>

<p>
所以要在Hadoop集群新建目录，执行命令
</p>

<pre class="example">
[manue1@master conf]$ hadoop fs -mkdir -p /user/hive/warehouse
[manue1@master conf]$ hadoop fs -chmod -R 777 /user/hive/warehouse
[manue1@master conf]$ hadoop fs -mkdir -p /tmp/hive
[manue1@master conf]$ hadoop fs -chmod -R 777 /tmp/hive

</pre></li>

<li><p>
hive-site.xml内mysql相关配置
</p>

<p>
需要java连接mysql的依赖包下载<a href="https://dev.mysql.com/downloads/connector/j/5.1.html">mysql-connector-java-5.1.45-bin.jar</a> 
</p>

<pre class="example">
mv mysql-connector-java-5.1.45-bin.jar lib/

</pre>

<pre class="example">
1. javax.jdo.option.ConnectionDriverName，将该name对应的value修改为MySQL驱动类路径：
&lt;property
  &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;
  &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;
&lt;/property&gt;  

2. javax.jdo.option.ConnectionURL，将该name对应的value修改为MySQL的地址：
 &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;
 &lt;value&gt;jdbc:mysql://192.168.56.11:3306/hive?createDatabaseIfNotExist=true&lt;/value&gt;

3.javax.jdo.option.ConnectionUserName，将对应的value修改为MySQL数据库登录名：
&lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;
&lt;value&gt;hive&lt;/value&gt;

4.javax.jdo.option.ConnectionPassword，将对应的value修改为MySQL数据库的登录密码：
&lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;
&lt;value&gt;*******&lt;/value&gt;

</pre></li>

<li><p>
替换${system 等值
</p>

<p>
${system:user.name}都替换为manue1
</p>

<p>
${system:java.io.tmpdir}替换为hive的临时目录 /home/manue1/opt/apache-hive-2.3.2-bin/iotmp,先创建，再替换
</p>

<pre class="example">
[manue1@master apache-hive-2.3.2-bin]$ mkdir iotmp
[manue1@master apache-hive-2.3.2-bin]$ sudo chmod -R 777 iotmp

</pre>

<pre class="example">
:%s/${system:java.io.tmpdir}/\/home\/manue1\/opt\/apache-hive-2.3.2-bin\/iotmp/gg
:%s/${system:user.name}/manue1/gg

</pre></li>
</ol></li>
</ul>

<p>
最后初始化metadata表数据
</p>

<pre class="example">
schematool -initSchema -dbType mysql

</pre>
</div>
</div>

<div id="outline-container-orgd8a5063" class="outline-3">
<h3 id="orgd8a5063"><span class="section-number-3">5.3</span> start hive</h3>
<div class="outline-text-3" id="text-5-3">
<p>
Hive的三种启动方式
</p>

<ol class="org-ol">
<li><p>
hive  命令行模式
</p>

<p>
进入hive安装目录，输入bin/hive的执行程序，或者输入 hive &#x2013;service cli
</p>

<p>
用于linux平台命令行查询，查询语句基本跟mysql查询语句类似
</p></li>

<li><p>
hive  web界面的启动方式
</p>

<p>
bin/hive &#x2013;service hwi 
</p>

<p>
用于通过浏览器来访问hive，感觉没多大用途，浏览器访问地址是：127.0.0.1:9999/hwi
</p></li>

<li><p>
hive  远程服务 (端口号10000) 启动方式
</p>

<p>
bin/hive &#x2013;service hiveserver2 &amp;
</p>

<p>
用java，python等程序实现通过jdbc等驱动的访问hive就用这种起动方式了，这个是程序员最需要的方式了
</p></li>
</ol>


<p>
此时可以使用beeline 测试jdbc连接
</p>

<pre class="example">
beeline -u jdbc:hive2://slave01:10000 -n manue1 -p mmanue1

</pre>


<p>
问题一:
</p>
<pre class="example">

Connecting to jdbc:hive2://master:10000/default
18/01/10 20:37:17 [main]: WARN jdbc.HiveConnection: Failed to connect to master:10000
Error: Could not open client transport with JDBC Uri: jdbc:hive2://master:10000/default: Failed to open new session: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.authorize.AuthorizationException): User: manue1 is not allowed to impersonate anonymous (state=08S01,code=0)
Beeline version 2.3.2 by Apache Hive
beeline&gt; 

分析 ： 访问权限问题

解决 ：在hdfs 的配置文件core-site.xml中加入如下配置，root为位置填入  User:*  ，etc   hadoop.proxyuser.eamon.hosts

 &lt;property&gt;
   &lt;name&gt;hadoop.proxyuser.manue1.hosts&lt;/name&gt;
   &lt;value&gt;*&lt;/value&gt;
 &lt;/property&gt;
 &lt;property&gt;
  &lt;name&gt;hadoop.proxyuser.manue1.groups&lt;/name&gt;
  &lt;value&gt;*&lt;/value&gt;
&lt;/property&gt;
</pre>

<p>
问题二:
</p>
<pre class="example">
ERROR 1045 (28000): Access denied for user 'hive'@'slave01' (using password: YES)

查看mysql.user表已经存在hive@%，但依然不能访问slave01,最终无解只能添加下面一条

grant all privileges on hive.* to hive@'%'identified by 'manue1';
flush privileges; 

</pre>
</div>
</div>
</div>
<div id="outline-container-orgefc5e17" class="outline-2">
<h2 id="orgefc5e17"><span class="section-number-2">6</span> sqoop</h2>
<div class="outline-text-2" id="text-6">
<ol class="org-ol">
<li><p>
环境配置
</p>

<p>
从<a href="https://sqoop.apache.org">官网下载</a> 解压安装，配置SQOOP<sub>HOME目录</sub>
</p>
<pre class="example">
export SQOOP_HOME=/home/manue1/opt/sqoop-1.4.7.bin__hadoop-2.6.0
export PATH=$SQOOP_HOME/bin:$PATH
</pre>
<p>
拷贝\({SQOOP_HOME}/conf/sqoop-env-template.sh  
    到\){SQOOP<sub>HOME</sub>}/conf/sqoop-env.sh，
然后修改sqoop-env.sh
</p>
<pre class="example">
export HADOOP_COMMON_HOME=/home/manue1/opt/hadoop-2.7.5
export HADOOP_MAPRED_HOME=/home/manue1/opt/hadoop-2.7.5
export HIVE_HOME=/home/manue1/opt/apache-hive-2.3.2-bin
</pre></li>

<li><p>
测试连接mysql
</p>

<p>
将连接mysql的jar导入sqoop/lib内
</p>

<pre class="example">
sqoop list-databases --connect jdbc:mysql://slave01:3306/hive --username root --password manue1

</pre></li>
</ol>
</div>
</div>
<div id="outline-container-org92e5865" class="outline-2">
<h2 id="org92e5865"><span class="section-number-2">7</span> zookeeper</h2>
<div class="outline-text-2" id="text-7">
</div>
<div id="outline-container-orgdffecae" class="outline-3">
<h3 id="orgdffecae"><span class="section-number-3">7.1</span> Replicated ZooKeeper configural</h3>
<div class="outline-text-3" id="text-7-1">
<pre class="example">
note

For replicated mode, a minimum of three servers are required, and it is strongly recommended that you have an odd number of servers. If you only have two servers, then you are in a situation where if one of them fails, there are not enough machines to form a majority quorum. Two servers is inherently less stable than a single server, because there are two single points of failure.

  至少3节点,每个zookeeper服务都可以成为leader， follower，observer。
</pre>

<ul class="org-ul">
<li><p>
vi conf/zoo.cfg
</p>

<p>
sudo mkdir -p <i>var/lib/zookeeper
sudo chown manue1:manue1  /var/lib/zookeeper</i> #manue1 user start service
创建 vi /var/lib/zookeeper/myid 内容为node server.x ,如 master为1
</p>

<pre class="example">

tickTime=2000
dataDir=/var/lib/zookeeper
clientPort=2181
initLimit=5
syncLimit=2
server.1=master:2888:3888   #2888 集群互相通信 3888 leader选举
server.2=slave01:2888:3888
server.3=slave02:2888:3888

# ，端口 2181 由 ZooKeeper 客户端使用，用于连接到 ZooKeeper 服务器；端口 2888 由对等 ZooKeeper 服务器使用，用于互相通信；而端口 3888 用于领导者选举

</pre>
<p>
scp -r zookeeper-3.4.10 manue1@slave01:/home/manue1/opt
scp -r zookeeper-3.4.10 manue1@slave02:/home/manue1/opt
</p></li>

<li><p>
start service
</p>

<p>
nohup /home/manue1/opt/zookeeper-3.4.10/bin/zkServer.sh restart
</p>

<p>
ps -ef | grep zookeeper
</p>

<p>
netstat -tnlpa | grep 2181 
</p>

<p>
echo state | nc localhost 2181
</p></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org67b0d04" class="outline-2">
<h2 id="org67b0d04"><span class="section-number-2">8</span> hbase</h2>
<div class="outline-text-2" id="text-8">
<p>
start hadoop &amp; zookeeper
</p>
</div>
<div id="outline-container-org80aab88" class="outline-3">
<h3 id="org80aab88"><span class="section-number-3">8.1</span> hbase configural</h3>
<div class="outline-text-3" id="text-8-1">
<ul class="org-ul">
<li><p>
.bashrc
</p>

<pre class="example">
# Hbase Environment Variables
export HBASE_HOME=/home/manue1/opt/hbase-1.2.6/
PATH=$JAVA_HOME/bin:$HADOOP_HOME/bin:$HBASE_HOME/bin:$PATH



</pre></li>

<li><p>
hbase-env.sh
</p>
<pre class="example">
export JAVA_HOME=/home/manue1/opt/jdk8
export HBASE_MANAGES_ZK=false

</pre></li>

<li>hbase-site.xml</li>

<li><p>
regionservers
</p>
<pre class="example">
master
slave01
slave02
</pre></li>
</ul>
</div>
</div>


<div id="outline-container-org6af7569" class="outline-3">
<h3 id="org6af7569"><span class="section-number-3">8.2</span> hbase start</h3>
<div class="outline-text-3" id="text-8-2">
<p>
nohup /home/manue1/opt/hbase-1.2.6/bin/start-hbase.sh &amp;
</p>

<p>
nohup /home/manue1/opt/hbase-1.2.6/bin/stop-hbase.sh &amp;
</p>


<p>
<a href="http://master:16010/master-status">http://master:16010/master-status</a>
</p>
</div>
</div>
</div>
<div id="outline-container-org3406069" class="outline-2">
<h2 id="org3406069"><span class="section-number-2">9</span> kafka</h2>
<div class="outline-text-2" id="text-9">
</div>
<div id="outline-container-orgb7be641" class="outline-3">
<h3 id="orgb7be641"><span class="section-number-3">9.1</span> kafka configural</h3>
<div class="outline-text-3" id="text-9-1">
<ul class="org-ul">
<li><p>
conf/server.properties
</p>

<pre class="example">
#指定zookeeper的连接信息
zookeeper.connect=master:2181,slave01:2181,slave02:2181

#每个broker相当于一个节点，注意各个节点的broker.id的值必须唯一
broker.id=0

#broker监听端口
listeners=PLAINTEXT://master:9092

log.dir=/var/log/kafka
</pre>
<p>
sudo mkdir -p /var/log/kafka  
sudo chown manue1:manue1 /var/log/kafka
</p>

<p>
scp -r kafka<sub>2.11</sub>-1.0.0/ manue1@slave01:/home/manue1/opt
scp -r kafka<sub>2.11</sub>-1.0.0/ manue1@slave02:/home/manue1/opt
各个节点修改broker.id 和 listeners
</p></li>
</ul>
</div>
</div>

<div id="outline-container-orga80f540" class="outline-3">
<h3 id="orga80f540"><span class="section-number-3">9.2</span> kafka start service</h3>
<div class="outline-text-3" id="text-9-2">
<ul class="org-ul">
<li><p>
vi .bashrc
</p>
<pre class="example">
# KAFKA_HOME Environment Variables
export KAFKA_HOME=/home/manue1/opt/kafka_2.11-1.0.0
export PATH=$PATH:$KAFKA_HOME/bin

</pre></li>
</ul>


<p>
nohup  $KAFKA<sub>HOME</sub>/bin/kafka-server-start.sh $KAFKA<sub>HOME</sub>/config/server.properties &amp;
</p>

<p>
$KAFKA<sub>HOME</sub>/bin/kafka-server-stop.sh
</p>
<pre class="example">

   虚拟机环境内存不够，配置启动脚本 jvm heap 大小 

[manue1@slave01 config]$ $KAFKA_HOME/bin/kafka-server-start.sh $KAFKA_HOME/config/server.propertie
Java HotSpot(TM) 64-Bit Server VM warning: INFO: os::commit_memory(0x00000000c0000000, 1073741824, 0) failed; error='Cannot allocate memory' (errno=12)


vi kafka-server-start.sh

KAFKA_HEAP_OPTS="-Xmx256M -Xms128M
</pre>

<p>
sudo netstat -anp | grep 9092
</p>
</div>
</div>
<div id="outline-container-orgfafc201" class="outline-3">
<h3 id="orgfafc201"><span class="section-number-3">9.3</span> use kafka</h3>
<div class="outline-text-3" id="text-9-3">
<p>
<a href="http://blog.csdn.net/u010297957/article/details/72758765">http://blog.csdn.net/u010297957/article/details/72758765</a>
</p>

<p>
producer &amp; consumer
</p>
<pre class="example">

[manue1@master ~]$ $KAFKA_HOME/bin/kafka-topics.sh --create --topic TestTopic001 --partitions 2 --replication-factor 1 --zookeeper master:2181,slave01:2181,slave02:2181
Created topic "TestTopic001".

[manue1@master ~]$ $KAFKA_HOME/bin/kafka-topics.sh --describe --topic TestTopic001 --zookeeper master:2181,slave01:2181,slave02:2181
Topic:TestTopic001	PartitionCount:2	ReplicationFactor:1	Configs:
    Topic: TestTopic001	Partition: 0	Leader: 1	Replicas: 1	Isr: 1
    Topic: TestTopic001	Partition: 1	Leader: 2	Replicas: 2	Isr: 2

[manue1@master logs]$ $KAFKA_HOME/bin/kafka-console-producer.sh --broker-list master:9092,slave01:9092,slave02:9092 --topic TestTopic001
&gt;hi zbr
&gt;what is your name
&gt;i love zbr


[manue1@slave01 kafka_2.11-1.0.0]$ $KAFKA_HOME/bin/kafka-console-consumer.sh --from-beginning --topic TestTopic001 --zookeeper master:2181,slave01:2181,slave02:2181
Using the ConsoleConsumer with old consumer is deprecated and will be removed in a future major release. Consider using the new consumer by passing [bootstrap-server] instead of [zookeeper].
hi zbr
what is your name
i love zbr

</pre>
</div>
</div>
</div>
<div id="outline-container-org0076e18" class="outline-2">
<h2 id="org0076e18"><span class="section-number-2">10</span> spark</h2>
<div class="outline-text-2" id="text-10">
</div>
<div id="outline-container-org4fd5fec" class="outline-3">
<h3 id="org4fd5fec"><span class="section-number-3">10.1</span> spark configural</h3>
<div class="outline-text-3" id="text-10-1">
<p>
<a href="https://data-flair.training/blogs/install-apache-spark-multi-node-cluster/">https://data-flair.training/blogs/install-apache-spark-multi-node-cluster/</a>
</p>


<ul class="org-ul">
<li><p>
.bashrc
</p>
<pre class="example">
#SPARK_HOME Environment Variables
export SPARK_HOME=/home/manue1/opt/spark-2.2.1-bin-hadoop2.7
export PATH=$PATH:$SPARK_HOME/bin
</pre></li>

<li><p>
spark-env.sh
</p>

<p>
cp spark-env.sh.template spark-env.sh
</p>

<pre class="example">
export JAVA_HOME=/home/manue1/opt/jdk8

export SCALA_HOME=/home/manue1/opt/scala-2.11.7

export HADOOP_HOME=/home/manue1/opt/hadoop-2.7.5

export HADOOP_CONF_DIR=/home/manue1/opt/hadoop-2.7.5/etc/hadoop

export SPARK_MASTER_IP=master

export SPARK_WORKER_MEMORY=64m

export SPARK_WORKER_CORES=1

export SPARK_WORKER_INSTANCES=2

变量说明
JAVA_HOME：Java安装目录
SCALA_HOME：Scala安装目录
HADOOP_HOME：hadoop安装目录
HADOOP_CONF_DIR：hadoop集群的配置文件的目录
SPARK_MASTER_IP：spark集群的Master节点的ip地址
SPARK_WORKER_MEMORY：每个worker节点能够最大分配给exectors的内存大小
SPARK_WORKER_CORES：每个worker节点所占有的CPU核数目
SPARK_WORKER_INSTANCES：每台机器上开启的worker节点的数目

</pre></li>

<li><p>
slaves
</p>
<pre class="example">
master
slave01
slave02
</pre></li>
</ul>


<p>
slave sync
</p>

<p>
scp -r spark-2.2.1-bin-hadoop2.7/ manue1@192.168.1.109:/home/manue1/opt
</p>
</div>
</div>
<div id="outline-container-orgf5bd40a" class="outline-3">
<h3 id="orgf5bd40a"><span class="section-number-3">10.2</span> spark start</h3>
<div class="outline-text-3" id="text-10-2">
<p>
nohup sh /home/manue1/opt/hadoop-2.7.5/sbin/start-all.sh &amp;  #启动hdfs即可
</p>

<p>
$SPARK<sub>HOME</sub>/sbin/start-all.sh
</p>


<p>
WebUI
<a href="http://master:8080/">http://master:8080/</a>
</p>

<p>
spark-shell.sh 运行后可以访问后台执行的任务
<a href="http://master:4040/">http://master:4040/</a>
</p>

<p>
spark-submit yarn 管理方式
<a href="http://master:8088/">http://master:8088/</a>
</p>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p>
    <a href="">manue1</a> 搭建和维护，使用
    <a href="https://www.gnu.org/software/emacs/">Emacs</a>
    <a href="http://orgmode.org/">Org mode</a> 编辑和构建
</p>
<script src="https://www.manue1.site/css/jquery-2.1.3.min.js"></script>
<script src="https://www.manue1.site/css/main.js"></script>
<!-- <script src="http://www.langdebuqing.com/css/jquery-2.1.3.min.js"></script>
     <script src="http://www.langdebuqing.com/css/main.js"></script> -->
</div>
</body>
</html>