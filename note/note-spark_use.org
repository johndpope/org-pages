#+SETUPFILE: setup.org
#+TITLE: spark 学习
Spark快速大数据分析[美] 卡劳（Holden Karau）
* what is spark
  spark 是一个用来实现快速而通用的集群计算的平台
  - spark core
  - spark sql
  - spark streaming
  - MLlib
  - GraphX
  - 集群管理器
    - 自带简易调度器，独立调度器
    - Hadoop YARN
    - Apache Mesos
  - Spark的存储层次
    - 支持本地文件,HDFS,Hive,HBase
    - 文件格式:文本文件,SequenceFile,Avro,Parquet
  - shell
    spark shell 可用来与分布式存储在许多机器的内存或者硬盘上的数据进行交互
    python shell / scala shell
  - SparkContext
    每个Sparkyingy
* RDD
  RDD(resilient distributed dataset)
** RDD基础
   Spark中的RDD是一个不可变的分布式对象集合
   
* spark use
** spark base
   rdd(resilient distributed dataset)
** spark install
   函数式编程
   shell下执行:paste
** spark shell
   bin/spark-shell
   - 创建RDD
     sc.parallelize()
   - RDD操作
     + 转化操作
       返回的是RDD
       map()
       filter()
     + 行动操作
       返回结果或把结果写入外部系统的操作,返回的是其他数据类型
       
** idea spark scala
   scala 项目手动打包
   删除scala 和spark的包

   执行中 spark-submit --class com.nti.spark.SparkTags spark_scala_jar.jar

   sbt 构建项目 需要等待sbt版本下载好才会生成src目录
   
** idea spark java 
   1. jre 1.5 换成1.8
** rdd & dataframe
* docker spark
** docker for mac install spark 2.2.0 + hadoop 2.8.0

  refer : http://blog.csdn.net/gogo_yao/article/details/76863201
  
  1. 启动容器
    docker run --name centos-6 -tid centos:6 /bin/bash
  2. 复制安装包到centos容器
    docker cp scala-2.12.3.tgz 23288ad95817:/
    docker cp jdk-8u144-linux-x64.rpm 23288ad95817:/
    docker cp spark-2.2.0-bin-without-hadoop.tgz 23288ad95817:/
    docker cp hadoop-2.8.0.tar.gz 23288ad95817:/root/.local
  3. 进入容器
     docker exec -it 23288ad9581 /bin/bash
     hadoop spark install
  4. 保存镜像
     docker commit -m "centos-6 with spark 2.2.0 and hadoop 2.8.0" 23288ad95817 centos:with-saprk-hadoop

     REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
     centos              with-saprk-hadoop   30333b7011ce        20 seconds ago      3.84GB
  5. 使用保存的镜像搭建集群
     
     #端口50070和8088，8080是用来在浏览器中访问hadoop、yarn、spark的WEB界面，这里分别映射到物理机的50070和8088，8080端口。
     docker run -itd -P -p 50070:50070 -p 8088:8088 -p 8080:8080 --name master -h master --add-host slave01:172.17.0.4 --add-host slave02:172.17.0.5 centos:with-saprk-hadoop
     docker run -itd -P --name slave01 -h slave01 --add-host master:172.17.0.3 --add-host slave02:172.17.0.5  centos:with-saprk-hadoop
     docker run -itd -P --name slave02 -h slave02 --add-host master:172.17.0.3 --add-host slave01:172.17.0.4  centos:with-saprk-hadoop
  6. 进入集群管理
     docker exec -it master /bin/bash
     
     service sshd restart

     cd /root/.local/hadoop-2.8.0/sbin;sh start-all.sh   
     cd /root/.local/spark-2.2.0-bin-without-hadoop/sbin; sh start-all.sh
** docker search spark
   docker pull sequenceiq/spark:1.6.0
* docker hbase
